---
title: "Data Processing with Spark"
format: html
execute:
  echo: true
  warning: true
  message: true
jupyter: python3
category: "data processing"
author: "Minkun Kim"
date: "10.28.2025"
categories:
  - data processing
comments:
  utterances:
    repo: "mainkoon81.github.io/minkun-website/"
---

## Spark vs Hadoop
“Imagine you have a huge pile of Lego pieces and you want to build lots of different things.
**Hadoop** writes down every single step in a notebook (disk I/O) before it adds even one Lego piece. It builds a little bit, writes it down, builds a little more, writes again. This makes sure it doesn’t forget anything, but it also means it is very slow because it keeps stopping to write.

**Spark** works with a super strong memory. It can hold lots of steps in its head without writing them down all the time, so it builds things much faster. Spark also supports richer workloads (counting, searching, learning, etc.) in a single framework, including SQL, streaming, machine learning, etc. This means it can switch between tasks easily — one moment it’s sorting Lego colors, next it’s counting pieces, next it’s building a robot — all without slowing down. Hadoop needs a whole new notebook step each time it changes what it’s doing.

![Spark Architecture & two main actors
](images/spark structure.png){width="1000"}

In Spark, the **Driver Program** is the central coordinator of a job. It constructs the execution plan, schedules tasks, and maintains the metadata and state of the application. The **Executors** are the distributed workers that actually run those tasks, store data in memory, and return results back to the Driver. Overseeing the whole system is the Cluster Manager—such as YARN, Kubernetes, or the standalone manager—which allocates resources to the application by deciding how many executors to launch and on which nodes. Together, the Driver provides the control logic, the Executors provide the computational power, and the Cluster Manager provides the resources needed for distributed execution. 





<!--

## Bibliography
\[1\] I. Kobyzev, S. J. D. Prince and M. A. Brubaker, "Normalizing
Flows: An Introduction and Review of Current Methods," in IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no.
11, pp. 3964-3979.

\[2\] Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G. (2019).
Neural Spline Flows. Advances in Neural Information Processing Systems,
abs/1906.04032.
https://proceedings.neurips.cc/paper/2019/hash/7ac71d433f282034e088473244df8c02-Abstract.html

\[3\] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & Kothe, U.
(2022). BayesFlow: Learning Complex Stochastic Models With Invertible
Neural Networks. IEEE Transactions on Neural Networks and Learning
Systems, 33(4), 1452–1466.

-->
[
  {
    "objectID": "whiteboard04/index.html",
    "href": "whiteboard04/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView my artwork (+ download link)"
  },
  {
    "objectID": "whiteboard02/POST/project_2/index.html",
    "href": "whiteboard02/POST/project_2/index.html",
    "title": "Quick note on AWS",
    "section": "",
    "text": "Normalising flow is commonly used for density estimation in generative model. It is also used in Simulatoin-Based Inference (SBI) to estimate conditional density. Specifically, \\(p(\\theta | \\mathcal{D})\\) for neural posterior estimation (NPE) and \\(p(\\mathcal{D} | \\theta)\\) for neural likelihood estimation (NLE). In this post, I will quickly explain how normalizing flow can estimate density with some focus on the application in SBI.\nIn generative model we are interested in estimating distribution of \\(x \\in \\mathbb{R}^D\\) by integrating out latent variable \\(z \\in \\mathbb{R}^D\\), that is\n\\[\np(x) = \\int p(x | z)p_z(z)dz\n\\]\nThe integral is usually intractable. In normalizing flow, we pick \\(p_z(z)\\) to be a simple distirbution such as standard normal distribution \\(N(0, I_D)\\), and transform \\(p_z(z)\\) with \\(g: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D\\). We pick \\(g\\) to be diffeomorphism, that is, \\(g\\) is 1. bijective, 2. differentiable, 3. its inverse is differentiable. This allows us to express \\(p(x)\\) using change of variable formula (without any intractable integral involved)\n\\[\np(x) = p_z(g^{-1}(x)) \\left|\\det  \\frac{\\partial g^{-1}(x)}{\\partial x} \\right|\n:=  p_z(f(x)) \\left|\\det  \\frac{\\partial f(x)}{\\partial x} \\right|\n\\] We just defined \\(f := g^{-1}\\). \\(g\\) is referred as generative path because we try to generate \\(x\\) from a simple distribution \\(p_z\\), whereas \\(f\\) is referred as normalising path because we try to degenerate \\(x\\) back to \\(z \\sim p_z\\), where \\(p_z\\) is usually normal distribution.\n\\(f\\) is usually parameterized by neural network to maximise likelihood \\(p(x)\\). \\(f\\) is usually chosen such that the determinant of the Jacobian is easy to calculate. Training and evaluation of density requires normalising path while sampling requires generative path as\n\\[\nx \\sim p(x)\n\\mathrel{\\Leftrightarrow}\nz \\sim p_z(z), x = g(z)\n\\]\nPeople often care about \\(f\\) because fast evaluation of \\(f\\) is needed for fast training. If we need fast sampling, then \\(g\\) is cared.\nIn SBI, coupling flow and autoregressive flow is commonly used."
  },
  {
    "objectID": "whiteboard02/POST/project_2/index.html#coupling-flow",
    "href": "whiteboard02/POST/project_2/index.html#coupling-flow",
    "title": "Quick note on AWS",
    "section": "Coupling flow",
    "text": "Coupling flow\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi &= NN(x^B) \\\\\ny^A &= \\mathbf{h}_\\phi(x^A) = [h_{\\phi_1}(x_1),...,h_{\\phi_d}(x_d)]  \\\\\ny^B &= x^B \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nFirstly, \\(x\\) is split into two dimension such that \\(x^A \\in \\mathbb{R}^d, x^B \\in \\mathbb{R}^{D - d}\\). Then one part is fed into NN: \\(\\mathbb{R}^{D - d} \\rightarrow \\mathbb{R}^k\\) to learn parameter \\(\\phi \\in  \\mathbb{R}^k\\). The NN is referred as conditioner. This \\(\\phi\\) is then coupled with another part \\(x^A\\) through invertible and differentiable function \\(\\mathbf{h}\\) called transformer or coupling function. \\(k\\) depends on the choice of coupling function. Note that transformer is nothing to do with the ‘query-key-value’ transformers. Coupling function is usually applied element-wise. This transformed \\(x^A\\) is then concatenated with \\(x^B\\), which gives output \\(y\\). Its inverse is given by\n\\[\n\\begin{align}\nx^B &= y^B \\\\\n\\phi &= NN(x^B) \\\\\nx^A & = \\mathbf{h}_{\\phi}^{-1}(y^A)\n\\end{align}\n\\]\nThis ‘coupling’ is repeated multiple times, combined with non-stochastic permutation such that all the dimensions \\(i = 1,...,D\\) are interacted to each other. The determinant of Jacobian is given by \\(\\prod_{i=1}^d \\frac{\\partial h_{\\phi_i}(x_i)}{\\partial x_i}\\).\nIn NPE, \\(x \\leftarrow \\theta\\) and the conditioner is also conditioned on data as \\(\\phi = NN(\\theta^B, \\mathcal{D})\\). In NLE, \\(x \\leftarrow \\mathcal{D}\\) and \\(\\phi = NN(\\mathcal{D}^B, \\theta)\\)"
  },
  {
    "objectID": "whiteboard02/POST/project_2/index.html#autoregressive-flow",
    "href": "whiteboard02/POST/project_2/index.html#autoregressive-flow",
    "title": "Quick note on AWS",
    "section": "Autoregressive flow",
    "text": "Autoregressive flow\n\\[\n\\begin{align}\n\\text{For } i &= 2,...,D:\\\\\n\\phi_i &= NN(x_{1:i-1}) \\\\\ny_i &= h_{\\phi_i}(x_i)\\\\\n\\end{align}\n\\]\nwhere \\(\\phi_1\\) is constant. This is very similar to coupling flow except that instead of splitting dimensions into half, dimension is split in autoregressive manners into \\(x_i\\) and \\(x_{1:i-1}\\) mutiple times. In practice, this coupling functions can be implemented without recursion using masking.\nIn NPE, \\(\\phi_1 = NN(\\mathcal{D}), \\phi_i = NN(\\theta_{1:i-1}, \\mathcal{D}), 1 &lt; i \\leq D\\). In NLE, \\(\\phi_1 = NN(\\theta),\\phi_i = NN(\\mathcal{D}_{1:i-1}, \\theta), 1 &lt; i \\leq D\\)"
  },
  {
    "objectID": "whiteboard02/POST/project_2/index.html#coupling-function",
    "href": "whiteboard02/POST/project_2/index.html#coupling-function",
    "title": "Quick note on AWS",
    "section": "Coupling function",
    "text": "Coupling function\n\nAffine\n\\[\nh_{\\phi_{i}}(x_i) = \\alpha_i x_i + \\beta_i, \\quad \\phi_i = (\\alpha_i, \\beta_i)\n\\]\nwhere \\(\\alpha_i &gt; 0, \\beta_i \\in \\mathbb{R}\\). For affine coupling function, \\(k = d \\times 2\\).  \n\n\nMonotonic rational quadratic spline\nIn neural spline flow, monotonic rational quadratic spline (RQ-spline) is used for \\(h\\). It allows to model more expressive transformation from \\(x\\) to \\(y\\) and to \\(z\\) at final. Monotonicity is necessary for bijectivity. In a nutshell, \\(h\\) is defined in the interval \\([-B, B]\\) and that interval is split into \\(K\\) bins using \\(K + 1\\) knots, where \\(B\\) and \\(K\\) are hyper-parameters. The height and the width of the bins are trainable parameters \\((\\phi_{h,j}, \\phi_{w, j}), j = 1,...,K\\), and derivatives at each knots are also trainable parameters \\(\\phi_{d,j}, j = 1,...,K-1\\). These parameters defines the RQ-spline. In total, the coupling function have \\(k = (3K - 1) \\times d\\) parameters.\n\n\n\nMonotonic RQ-spline. Taken from [2]\n\n\nIn this figure, \\(K = 10\\) is picked."
  },
  {
    "objectID": "whiteboard02/POST/project_2/index.html#bibliography",
    "href": "whiteboard02/POST/project_2/index.html#bibliography",
    "title": "Quick note on AWS",
    "section": "Bibliography",
    "text": "Bibliography\n[1] I. Kobyzev, S. J. D. Prince and M. A. Brubaker, “Normalizing Flows: An Introduction and Review of Current Methods,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 3964-3979.\n[2] Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G. (2019). Neural Spline Flows. Advances in Neural Information Processing Systems, abs/1906.04032. https://proceedings.neurips.cc/paper/2019/hash/7ac71d433f282034e088473244df8c02-Abstract.html\n[3] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & Kothe, U. (2022). BayesFlow: Learning Complex Stochastic Models With Invertible Neural Networks. IEEE Transactions on Neural Networks and Learning Systems, 33(4), 1452–1466."
  },
  {
    "objectID": "whiteboard02/POST/project_2/index.html#appendix",
    "href": "whiteboard02/POST/project_2/index.html#appendix",
    "title": "Quick note on AWS",
    "section": "Appendix",
    "text": "Appendix\n\nDual coupling\nSometimes, one block of coupling is defined as two coupling as in [3] such that both split is transformed by coupling function in one unit of coupling.\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi_1 &= NN_1(x^B) \\\\\ny^A &= \\mathbf{h}_{\\phi_1}(x^A) \\\\\n\\phi_2 &=  NN_2(y^A)\\\\\ny^B &= \\mathbf{h}_{\\phi_2}(x^B) \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nThis is same as applying single coupling flow followed by permutation, which swap \\(x^A\\) and \\(x^B\\) and applying another coupling flow.\n\n\nWhen \\(D = 1\\)\nIn generative model \\(x\\) is usually something high dimensional like image. But in SBI, \\(D\\) could be 1. Coupling flow could be applied in such case. For example in NPE, if \\(\\theta \\in \\mathbb{R}\\) and consider only one coupling, then we have\n\\[\n\\begin{align}\n\\phi &= NN(\\mathcal{D}) \\\\\nz &= \\mathbf{h}_{\\phi}(\\theta)\n\\end{align}\n\\]\nIn this case, \\(\\theta\\) is not going to be coupled with another split of \\(\\theta\\) because \\(\\theta\\) is just one-dimensional, but it is still coupled with \\(\\mathcal{D}\\).\nIn this case, naturally, coupling flow is same as autoregressive flow given same conditioner and coupling function."
  },
  {
    "objectID": "whiteboard02/index.html",
    "href": "whiteboard02/index.html",
    "title": "Data Engineering",
    "section": "",
    "text": "In energy finance, being a quant isn’t just about building models — it’s about making sure those models are fed with clean, timely, and scalable data. Our models are only as good as the data that drives them. Model accuracy depends on data quality, not just mathematical skill! We are dealing with massive amounts of real-time and historical data — electricity load curves, weather forecasts, fuel prices, grid constraints, power plant outages, carbon credit prices, etc. Without a well-designed ETL process or pipeline, this data is messy, delayed, or incomplete, making any model unreliable. That’s where data engineering skills come in.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Pipelining with AWS\n\n\n\nAWS\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\n\n\n\n\n\n\n\nQuick note on AWS\n\n\n\nAWS\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "My Research Work",
    "section": "",
    "text": "PhD Thesis: Risk Premium Development with Model Risk\n\nDCU DORAS | Code | \nMotivated by the case of catastrophic financial losses in the insurance industry, this thesis investigates Bayesian approaches to address diverse model risks in risk premium prediction. Unlike classical actuarial methods that rely solely on data, Bayesian models incorporate parameter knowledge, offering flexibility in handling erroneous data issue. We leverage this advantage to link Bayesian parametric/nonparametric frameworks with state-of-art strategies for managing incomplete data issues, such as Missingness at Random (MAR) and Non-Differential Berkson (NDB) mismeasurement. Additionally, we address other key analytical challenges, including heterogeneity, convolution, and scalability.\n\n\n\nBayesian Nonparametric Log Skew-Normal Mixture with Missing At Random Covariates in Insurance Practice\n\nEconometrics | Code\nWe propose novel connections among a covariate-dependent Dirichlet process mixture, log-normal convolution, and missing covariate imputation. As a generative approach, our framework models the joint of outcome and covariates, which allows us to impute missing covariates under the assumption of missingness at random.\n\n\n\nParametric Hierarchical Bayes with Non-Differential Berkson Covariates in Insurance Practice\n\nApplied Sciences | Code\nWe explore connections between Bayesian hierarchical modeling, partial pooling techniques, and the Gustafson correction method for mismeasured covariates. Focusing on Non-Differential Berkson (NDB) mismeasurement, we propose an approach that corrects such errors without relying on gold standard data. We discover the unique prior knowledge regarding the variance of the NDB errors, and utilize it to adjust the biased parameter estimates built upon the NDB covariate.\n\n\n\nUsing Tableau and Google Map API for Understanding the Impact of Walkability on Dublin City\n\narXiv Preprint | Code\nWe utilize the Google Map API and Tableau to visualize the less walkable areas across Dublin city and using WLS regression, we assess the effects of unwalkability on house prices in Dublin, thus quantifying the importance of walkable areas from an economic perspective."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Minkun Kim",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \n\nWelcome to my website!\nI am a scholar dedicated to understanding the complexities of our stochastic world. This website offers a window into my research, publications, and selected findings accumulated over years of academic and applied work. My research interests lie at the intersection of Bayesian computing, quantitative finance, data engineering, and energy markets. I also have a personal interest in data-driven urban planning. Here, you’ll find a selection of my published work, along with recent insights and ongoing developments from my current studies.\nPlease feel free to contact me if you have any questions or would like to discuss potential projects.\n\n\n\nInterests\nValuation/Risk Modeling  Energy Derivative Pricing  Stochastic Modeling  Nonparametric Bayesian Computing  Data Engineering  Data-Driven Architectural Urbanism \n\n\nEducation\n\nPh.D in Computing  Dublin City University, Ireland \nM.Sc in Financial Mathematics  University of York, UK \nM.Sc in Sustainable Energy Finance  Ruhr University Bochum, Germany \nM.Sc in Data Analytics  Dublin City University, Ireland \nB.Sc in Urban Engineering  Hongik University, South Korea"
  },
  {
    "objectID": "whiteboard01/index.html",
    "href": "whiteboard01/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world."
  },
  {
    "objectID": "whiteboard02/POST/project_1/index.html",
    "href": "whiteboard02/POST/project_1/index.html",
    "title": "Data Pipelining with AWS",
    "section": "",
    "text": "Data Pipeline Architecture & Roadmap.\nIn this project, we introduce a cloud-based data pipelining using AWS. We collect “Korean housing price” data from a public API (https://apis.data.go.kr/1613000/RTMSDataSvcAptTrade/getRTMSDataSvcAptTrade?LAWD_CD=11110&DEAL_YMD=202407&serviceKey=…..) and store the raw data in an ‘S3’ data lake storage. Using Python scripts running on AWS ‘EC2’ (computing resource), we process the data — for example, deriving building prices — and send the results back to ‘S3’.\nThe pipeline then performs ETL operations using AWS ‘Lambda’ for lightweight tasks and AWS ‘Glue’ for larger-scale processing. The cleaned and transformed data is stored in a separate ‘S3’ bucket. We register the processed data in the AWS ‘Glue Data Catalog’ so it can be queried directly with AWS ‘Athena’ using SQL.\nIn parallel, we load a structured version of the data into AWS ‘Redshift’ as our data warehouse to support downstream analytics. For visualization and reporting, we use AWS ‘QuickSight’ to connect to ‘Redshift’ and generate dashboards and insights.\nOnce this pipeline is in place, it can easily be extended — for example, by adding new API sources or additional transformation steps. While the current pipeline is not yet fully automated, we plan to orchestrate the workflows using Apache Airflow so it can run on a daily, hourly, or monthly basis."
  },
  {
    "objectID": "whiteboard02/POST/project_1/index.html#step-01.-data-collection",
    "href": "whiteboard02/POST/project_1/index.html#step-01.-data-collection",
    "title": "Data Pipelining with AWS",
    "section": "Step 01. Data Collection",
    "text": "Step 01. Data Collection\nFirst, we develop an API integration to retrieve real-time housing price data using a secured API key. The API data ingestion can be facilitated using the software “Postman.com” for testing and validation. Once the HTTP requests return the data, we collect and store the raw responses in AWS S3 data lake for further processing.\n\n\n\nAPI Scraping Examples: fetching XML data using API Call with key-based authentication - The sample result shows 10 records in each page and 60 items in total.\n\n\nIn AWS, we first create an S3 bucket and a designated folder to store the raw data. The following Python script is used to retrieve data from the API and perform initial processing. By “processing the raw data,” it mainly mean extracting the required columns from the XML response and uploading the collected dataset to the S3 data lake.\n\n\nCode\n#######################################################################################\n# function 00\ndef processing_datas(root):\n    \"\"\"\n        data structure\n        [{transaction #1}, {transaction #2}]\n    \"\"\"\n    list_temp = list()\n\n    for item in root.iter(\"item\"):\n        dict_item = {\n            \"sggCd\": item.find(\"sggCd\").text,\n            \"umdNm\": item.find(\"umdNm\").text,\n            \"aptNm\": item.find(\"aptNm\").text,\n            \"jibun\": item.find(\"jibun\").text,\n            \"excluUseAr\": item.find(\"excluUseAr\").text,\n            \"dealYear\": item.find(\"dealYear\").text,\n            \"dealMonth\": item.find(\"dealMonth\").text,\n            \"dealDay\": item.find(\"dealDay\").text,\n            \"dealAmount\": item.find(\"dealAmount\").text,\n            \"floor\": item.find(\"floor\").text,\n            \"buildYear\": item.find(\"buildYear\").text,\n            \"cdealType\": item.find(\"cdealType\").text,\n            \"cdealDay\": item.find(\"cdealDay\").text,\n            \"dealingGbn\": item.find(\"dealingGbn\").text,\n            \"estateAgentSggNm\": item.find(\"estateAgentSggNm\").text,\n            \"rgstDate\": item.find(\"rgstDate\").text,\n            \"aptDong\": item.find(\"aptDong\").text,\n            \"slerGbn\": item.find(\"slerGbn\").text,\n            \"buyerGbn\": item.find(\"buyerGbn\").text,\n            \"landLeaseholdGbn\": item.find(\"landLeaseholdGbn\").text\n        }\n        list_temp.append(dict_item)\n\n    return list_temp\n\n\n#: pip install requests \n#######################################################################################\n# function 01\ndef get_apt_trade_from_api(service_key: str, lawd_cd: str, deal_ymd: str):\n    import xml.etree.ElementTree as ET\n    import requests\n\n    list_result = list()\n    num_of_rows = 50\n    page_no = 1\n\n\n    while True:\n        end_point_url = (\"http://apis.data.go.kr/1613000/RTMSDataSvcAptTrade/getRTMSDataSvcAptTrade?\"\n                         f\"serviceKey={service_key}&LAWD_CD={lawd_cd}&DEAL_YMD={deal_ymd}\"\n                         f\"&numOfRows={num_of_rows}&pageNo={page_no}\")\n\n        response = requests.get(end_point_url)\n        root = ET.fromstring(response.text)\n        total_count = int(root.find(\"body/totalCount\").text)\n\n        # 2. data processing\n        list_result += processing_datas(root)\n\n        # Pagination\n        if len(list_result) &gt;= total_count:\n            break\n\n        page_no += 1\n\n    return list_result\n\n\n#: pip install jsonlines\n#######################################################################################\n# function 02\ndef save_file(content, file_name, file_type):\n    \"\"\"\n        data structure\n        { transaction #1 info }\n        { transaction #2 info }\n        ...\n        { transaction #n info }\n    \"\"\"\n    import jsonlines\n\n    if file_type == \"json\":\n        with jsonlines.open(file_name, \"w\") as f:\n            f.write_all(content)\n\n\n#: pip install boto3\n#######################################################################################\n# function 03\ndef upload_to_s3(file_name, bucket_name, object_name):\n    from botocore.exceptions import NoCredentialsError\n    import boto3\n\n    s3_client = boto3.client(\"s3\")\n\n    try:\n        s3_client.upload_file(file_name, bucket_name, object_name)\n    except NoCredentialsError:\n        print(\"Cannot found AWS permission.\")\n\n\n\n#################################################################################################\n#################################################################################################\n#: pip install python-dotenv \n#&gt; Notice: create \".env\" file in advance and save our SERVICE_KEY from the API (for security reason..) \n#################################################################################################\n#################################################################################################\ndef main():\n    # prepare to bring your SERVICE_KEY\n    from dotenv import load_dotenv\n    import os\n\n    # set the variable for SERVICE_KEY\n    load_dotenv()\n    # bring ur service key\n    service_key = os.getenv(\"SERVICE_KEY\")\n\n    # output formatting\n    file_type = \"json\"\n    bucket_name = \"khousepricing\"\n\n    # bring essential \"key\" and \"value\" as listed in the API document\n    list_LAWD_CD = [\n        \"11110\"  # AREA_1\n        , \"11140\"  # AREA_2\n        , \"11170\"  # AREA_3\n        , \"11200\"  # AREA_4\n        , \"11215\"  # AREA_5\n        , \"11230\"  # AREA_6\n        , \"11260\"  # AREA_7\n        , \"11290\"  # AREA_8\n        , \"11305\"  # AREA_9\n        , \"11320\"  # AREA_10\n        , \"11350\"  # AREA_11\n        , \"11380\"  # AREA_12\n        , \"11410\"  # AREA_13\n        , \"11440\"  # AREA_14\n        , \"11470\"  # AREA_15\n        , \"11500\"  # AREA_16\n        , \"11530\"  # AREA_17\n        , \"11545\"  # AREA_18\n        , \"11560\"  # AREA_19\n        , \"11590\"  # AREA_20\n        , \"11620\"  # AREA_21\n        , \"11650\"  # AREA_22\n        , \"11680\"  # AREA_23\n        , \"11710\"  # AREA_24\n        , \"11740\"  # AREA_25\n    ]\n\n    for DEAL_YMD in [\"202401\"]:\n        for LAWD_CD in list_LAWD_CD:\n            file_name = f\"{DEAL_YMD}_{LAWD_CD}_result.json\"\n            object_name = f\"apt-trade-raw/DEAL_YMD={DEAL_YMD}/LAWD_CD={LAWD_CD}/result.json\"\n\n            # 1. fetch raw data from API\n            trade_result = get_apt_trade_from_api(service_key, LAWD_CD, DEAL_YMD)\n\n            # 3. data processing\n            save_file(trade_result, file_name, file_type)\n\n            # 4. save it as a file and store it in S3 datalake\n            upload_to_s3(file_name, bucket_name, object_name)\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\nAPI Scraping Results: raw data DEAL_YMD=202401/ delieverd by code “mycode.py”\n\n\nNote that, we create new file and name it as .env to store sensitive service keys as shown below that should not be hard-coded in the script.\n\nSERVICE_KEY= xxxxxxx\n\nThis is retrieved by service_key = os.getenv(“SERVICE_KEY”) in the code."
  },
  {
    "objectID": "whiteboard02/POST/project_1/index.html#step-02.-ec2-instance",
    "href": "whiteboard02/POST/project_1/index.html#step-02.-ec2-instance",
    "title": "Data Pipelining with AWS",
    "section": "Step 02. EC2 instance",
    "text": "Step 02. EC2 instance\nDEAL_YMD=202401/, DEAL_YMD=202402/, DEAL_YMD=202403/,… Now we’re running the code on the AWS EC2 instance. To launch the instance, we selected:\n\nOperating System: Ubuntu 24.04 LTS (HVM)\nInstance Type: t3.small (2 vCPUs, 2 GiB memory)\nKey Pair: Created to enable secure SSH access\n\nSince we are using a Windows laptop, we generated the key pair in “.ppk” format to use PuTTY. After completing the setup, we can see the new instance running in the AWS Management Console.\n\n\n\nEC2 instance in the AWS Management Console\n\n\nTo access the instance, first retrieve its public IP address and establish a connection between our local device and the EC2 instance on the AWS remote server (using PuTTY). Before proceeding, we verify that the instance’s security group allows inbound SSH traffic. If SSH is not enabled, we add a new inbound rule to allow “SSH access” to the server. Because our local device runs Windows, we use PuTTY to establish the connection.\nNext, we open a new PUTTY terminal window specifically for transferring all necessary files in the local device to the AWS EC2 instance using the pscp command, while keeping the PUTTY terminal connected to the AWS server.\nFor PuTTY users on Windows, 1)Open PuTTY. 2)Go to Session and Enter the Host Name as “public IP” ubuntu@16.170.170.76 for example. Finally, go to Connection to SSH and in Auth: Browse and select key file:“.ppk” that was offered by AWS EC2 instance.\nOn the EC2 instance, we will set up a Python 3 environment. To prepare for this, go back to our local terminal and first generate a requirements.txt file that contains all Python packages required to run the script mycode.py.\n\npip freeze &gt; requirements.txt\n\nThe following is to securely transfer the necessary files from the local machine to the EC2 instance (PuTTY):\n\npscp -i MyKeypairForEC2.ppk requirements.txt ubuntu@16.170.170.76:/home/ubuntu/\npscp -i MyKeypairForEC2.ppk .env ubuntu@16.170.170.76:/home/ubuntu/\npscp -i MyKeypairForEC2.ppk mycode.py ubuntu@16.170.170.76:/home/ubuntu/\n\n\n\n\nnew PuTTY terminal in local machine\n\n\nNote that in the example above: ubuntu@172.31.7.42 = house number inside the gated community (Private IP of EC2 instance) and ubuntu@16.170.170.76 = street address that the whole world can use to find you (Public IP of AWS)\nIn PUTTY terminal (using EC2 instance), we run “mycode.py” to store the raw data into the S3 data lake.\n\nCheck what’s inside: ls -lrt\ninstall: sudo apt install python3-pip\nupdate: sudo apt update\ncreate a virtual ENV 1: python3 -m venv ~/.venvs/myproj\ncreate a virtual ENV 2: . ~/.venvs/myproj/bin/activate\ninstall: pip3 install -r requirements.txt\ninstall: curl “https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip” -o “awscliv2.zip”\ninstall: unzip awscliv2.zip\ninstall: sudo ./aws/install\nconfigure on EC2: aws configure\n\nAWS Access Key ID: \nAWS Secret Access Key: \nDefault region: eu-north-1\nDefault output format: json\n\nRUN: python mycode.py\n\nNow, return to the S3 bucket and verify that the raw data has been successfully uploaded.\n\n\n\nRaw (JSON) data stored in S3 data lake using the file “mycode.py”"
  },
  {
    "objectID": "whiteboard02/POST/project_1/index.html#step-03.-etl",
    "href": "whiteboard02/POST/project_1/index.html#step-03.-etl",
    "title": "Data Pipelining with AWS",
    "section": "Step 03. ETL",
    "text": "Step 03. ETL\nWhen we want to process raw data stored in S3 (or other sources), clean it, transform it, and load it somewhere else (like S3, RDS, Redshift, DynamoDB), AWS gives us two options.\n\nAWS Lambda: Good for lightweight, event-driven ETL\nAWS Glue: Managed ETL service for larger workloads\n\nIn this project, we focus on executing large-scale ETL using AWS Glue. As the initial step, we build a Glue Data Catalog by running a Glue Crawler, which automatically infers table schemas from the raw data. To configure the Crawler, we add the data source by connecting to the S3 bucket through the AWS Console. The Crawler then scans the data, identifies the structure, and creates the corresponding table schemas and partitions.\nA Glue Data Catalog is ready for efficient processing of large-scale datasets.\n\n\n\nCrawler created a Data Catalog\n\n\n\n\n\nRaw tables drawn from a Data Catalog\n\n\nIn short,\n\nSource (to fetch): S3\nTransform: SQL queries\nTargets (to save): S3\n\nMoving on to the ETL process, we use SQL queries to clean and standardize the raw data. This includes renaming columns, converting string values to numeric types (e.g., float or integer), removing unnecessary commas, and replacing null values with zero. The SQL queries used here are presented as below.\n\n\nShow SQL Query\n\nSELECT sggCd AS sgg_cd\n     , case when sggCd = '11110' then 'AREA_01'\n            when sggCd = '11140' then 'AREA_02'\n            when sggCd = '11170' then 'AREA_03'\n            when sggCd = '11200' then 'AREA_04'\n            when sggCd = '11215' then 'AREA_05'\n            when sggCd = '11230' then 'AREA_06'\n            when sggCd = '11260' then 'AREA_07'\n            when sggCd = '11290' then 'AREA_08'\n            when sggCd = '11305' then 'AREA_09'\n            when sggCd = '11320' then 'AREA_10'\n            when sggCd = '11350' then 'AREA_11'\n            when sggCd = '11380' then 'AREA_12'\n            when sggCd = '11410' then 'AREA_13'\n            when sggCd = '11440' then 'AREA_14'\n            when sggCd = '11470' then 'AREA_15'\n            when sggCd = '11500' then 'AREA_16'\n            when sggCd = '11530' then 'AREA_17'\n            when sggCd = '11545' then 'AREA_18'\n            when sggCd = '11560' then 'AREA_19'\n            when sggCd = '11590' then 'AREA_20'\n            when sggCd = '11620' then 'AREA_21'\n            when sggCd = '11650' then 'AREA_22'\n            when sggCd = '11680' then 'AREA_23'\n            when sggCd = '11710' then 'AREA_24'\n            when sggCd = '11740' then 'AREA_25'\n            else ''\n       end as sgg_nm\n     , umdNm as umd_nm\n     , aptNm as apt_nm\n     , jibun\n     , cast(excluUseAr as float) as exclu_use_ar\n     , cast(dealYear as int) as deal_year\n     , cast(dealMonth as int) as deal_month\n     , cast(dealDay as int) as deal_day\n     , cast(replace(dealAmount, ',', '') as int) as deal_amount\n     , cast(floor as int) as floor\n     , cast(buildYear as int) as build_year\n     , case when cdealType == ' ' then null\n            else cdealType\n       end as cdeal_type\n     , case when cdealDay == ' ' then null\n            else cdealDay\n       end as cdeal_day\n     , case when dealingGbn == ' ' then null\n            else dealingGbn\n       end as dealing_gbn\n     , case when estateAgentSggNm == ' ' then null\n            else estateAgentSggNm\n       end as estate_agent_sgg_nm\n     , case when rgstDate == ' ' then null\n            else rgstDate\n       end as rgst_date\n     , case when aptDong == ' ' then null\n            else aptDong\n       end as apt_dong\n     , case when slerGbn == ' ' then null\n            else slerGbn\n       end as sler_gbn\n     , case when buyerGbn == ' ' then null\n            else buyerGbn\n       end as buyer_gbn\n     , case when landLeaseholdGbn == ' ' then null\n            else landLeaseholdGbn\n       end as land_leasehold_gbn\n     , deal_ymd\n     , lawd_cd\n  FROM apt-trade-raw\nIn AWS Athena, we can make SQL analysis directly on data stored in Amazon S3 without the need to set up a database or server infrastructure. Athena allows us to query structured, semi-structured, or raw data using standard SQL, making it a powerful tool for exploratory data analysis and validation after ETL processes."
  },
  {
    "objectID": "whiteboard03/index.html",
    "href": "whiteboard03/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world."
  }
]
[
  {
    "objectID": "whiteboard04/index.html",
    "href": "whiteboard04/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView my artwork (+ download link)"
  },
  {
    "objectID": "whiteboard03/POST/project_3/index.html",
    "href": "whiteboard03/POST/project_3/index.html",
    "title": "Double Materiality Assessment",
    "section": "",
    "text": "What is important for your sustainability reporting? While going through this, you will get to know your firm way deeper.\nIn sustainability reporting, there is no universal procedure that applies equally to all economic activities. Under ESRS, companies must first identify potentially relevant sustainability topics and then assess their relevance through a double materiality assessment (DMA).\nIn this process, the DMA acts as a filter that determines which sustainability topics are relevant and therefore require disclosure in accordance with ESRS II."
  },
  {
    "objectID": "whiteboard03/POST/project_3/index.html#a.-phase-1-context-analysis",
    "href": "whiteboard03/POST/project_3/index.html#a.-phase-1-context-analysis",
    "title": "Double Materiality Assessment",
    "section": "[A]. Phase 1 – Context analysis",
    "text": "[A]. Phase 1 – Context analysis\nIt gives an overview of\n\nfirm’s activities, location, products\nvalue chain (up/downstream interactions - supplier, firm, logistics, distributors, customers)\nstakeholder (those influenced by decision)\nothers (regulatory landscape, industry analysis (checking partner firms), etc.)\n\nAccording to the MAIG, AutoGroup GmbH should first prepare a context analysis. Key aspects of this analysis cover the firm’s activities and environment, the key stakeholders and the firm’s value chain.\n\n(a) Activity analysis:\n\ntopic_1: strategy and business environment of the firm\n\nThe firm’s strategy focuses on maintaining a broad customer base, expanding electric-mobility services, and improving operational efficiency. The firm aims to strengthen its after-sales business, develop digital sales and service channels, and invest in EV-related capabilities such as charging infrastructure and high-voltage technician training. Sustainability and regulatory compliance are becoming increasingly integrated into strategic planning. The business environment is shaped by strong regulatory pressure (e.g., EU climate rules, environmental and safety regulations), the transition to electric vehicles, ongoing digitalisation of sales and workshop processes, competitive pressure from other dealerships and OEM-controlled sales channels, and a tight labour market for skilled mechanics. These trends create both risks (e.g., compliance costs, changing revenue mix) and opportunities (e.g., EV service growth, efficiency improvements).\n\ntopic_2: product/service portfolio of the firm\n\nThe firm offers a broad portfolio of automotive products and services. Its core activities include the sale of new and used vehicles across multiple brands, complemented by financing and leasing options for private customers and corporate fleets. The firm also provides trade-ins, vehicle delivery services, and tailored mobility solutions for business clients. A significant share of revenues comes from after-sales services, including maintenance, repairs, diagnostics, bodywork, and paint shop operations. The company is expanding its portfolio to include electric-mobility services such as EV charging infrastructure, battery diagnostics, and high-voltage system repairs. Additional services include spare parts sales, fleet management, and customer support across the vehicle life cycle, making AutoGroup GmbH a full-service mobility provider.\n\ntopic_3: geographic locations\n\nThe firm operates primarily in Germany, with a network of approximately 25 dealership and service locations distributed across several metropolitan and regional areas. These sites include showrooms, workshops, body shops, and administrative offices. While its core activities are domestic, the company interacts with international supply chains through vehicle manufacturers and parts suppliers. Its customer base is predominantly local and regional, with operations focused on serving private customers, corporate fleets, and mobility partners within Germany.\n\n\n(b) Value chain analysis:\n\ntopic_1: Which stages of the value chain does the company control directly? Which are outsourced or partner-driven?\n\nThe following are directly controlled\n\nVehicle Sales: AutoGroup manages all new and used car retail activities at its own dealerships.\nAfter-Sales Services: Workshops, inspections, diagnostics, and repairs are conducted by company employees on-site.\nBody & Paint Operations: The company runs its own paint booths and bodywork facilities, handling associated processes internally.\nCustomer Service Processes: Customer interaction, consultation, and complaint handling are controlled directly.\n\nThe following are outsourced / partner-driven\n\nVehicle Manufacturing: All production is handled by OEMs, outside the company’s control.\nParts Supply & Logistics: OEMs and logistics partners manage upstream transport and inventory flows.\nFinancing & Leasing Services: Financial products are provided through banks or OEM financing units.\nWaste Disposal & Specialized Training: Hazardous waste contractors and external trainers deliver required services\n\n\ntopic_2: main inputs (vehicles, parts, energy, water, data, labor)?\n\nVehicles & Parts: OEM-supplied cars and components form the foundation of the company’s sales and service business.\nSkilled Labor: Mechanics, technicians, and sales staff are essential for both retail and workshop operations.\nEnergy & Water: Workshops, paint operations, and charging infrastructure rely heavily on electricity, heating, and water.\nChemicals & Data Systems: Paints, solvents, lubricants, and digital diagnostic data support core workshop processes.\n\ntopic_3: main outputs (products, services, waste, emissions)?\n\nVehicle Sales: New and used vehicles sold to private and corporate customers generate primary revenue.\nService Outputs: Repairs, maintenance, diagnostics, and bodywork services support ongoing customer needs.\nWaste Streams: Workshops produce hazardous and non-hazardous wastes such as oils, chemicals, and tires.\nEmissions & Energy Use Outcomes: Heating, electricity consumption, and workshop activities create operational emissions.\n\ntopic_4: geographic boundaries (Germany-only? EU imports? cross-border leasing)\n\nDomestic Operations: All dealership and workshop activities take place within Germany.\nInternational Supply Chains: Vehicles and parts originate from EU and global OEM manufacturing locations.\nCross-Border Dependencies: The company interacts with international financing or logistics partners even though operations are local.\nCustomer Market Boundaries: Sales and services focus exclusively on customers within Germany.\n\n\n(c) Stakeholder analysis & evaluation:\n\ntopic_1: Who are the key stakeholders?\n\nCustomers (Private and Corporate)\nEmployees and Apprentices\nOEMs and Suppliers\nRegulators and Local Authorities\n\ntopic_2: **Relevance** 1. Interest\n\nCustomers (Private and Corporate): high-quality servicing, transparent pricing, and trustworthy customer support\nEmployees and Apprentices: fair working conditions, safe workplaces, competitive pay, and opportunities for training and development\nOEMs and Suppliers: dealerships to meet brand standards, maintain sales volumes\nRegulators and Local Authorities: full compliance with environmental, safety, and consumer protection regulations across all sites\n\ntopic_3: **Relevance** 2. Influence\n\nCustomers (Private and Corporate): shaping revenue and service demand\nEmployees and Apprentices: shaping service quality and customer satisfaction\nOEMs and Suppliers: shaping product supply, technology development\nRegulators and Local Authorities: imposing operational requirements\n\nThe stakeholder evaluation is performed to identify the Relevance of stakeholder group. - The relevance is determined by the highest value offered for the interest & influence.\n\nThe relevance is compared to the internal knowledge about the stakeholder. If the relevance exceeds the knowledge, it is translated into risk, and further engagement is required…via Survey, Interview, Workshop, SNS, etc.\n\n\n\n\n\n\nTable 1. Example of Stakeholder Evaluation (high/medium/low) based on the Relevance of stakeholder groups."
  },
  {
    "objectID": "whiteboard03/POST/project_3/index.html#b.-phase-2-iro-identification",
    "href": "whiteboard03/POST/project_3/index.html#b.-phase-2-iro-identification",
    "title": "Double Materiality Assessment",
    "section": "[B]. Phase 2 – IRO identification",
    "text": "[B]. Phase 2 – IRO identification\nAs a next step, the MAIG proposes the identification of IROs. As we expect that the E1 Standard might be material for the firm, we want to focus on potentially material IROs in this area first.\n\n(a) IRO identification approaches:\n\nIn practice, there are different approaches for identifying IROs regarding the organizational levels.\n\nTop-down: Derivation from the central level of the (Group) specialist department or the central sustainability task force. This approach provides a fast and potentially cost effective way to determine IROs, however it may miss out on feedback from subsidiaries.\nBottom-up: Identification takes place at subordinate levels such as individual subsidiaries or business units with subsequent consolidation at the overall company/group level. This approach potentially offers the broadest spectrum of potential IROs. However, this may result in additional work at group level to aggregate results.\nMixed approach: A combination of both approaches. This provides insights from both levels and may be useful in providing subsidiaries with initial anchors in terms of potentially relevant IROs. This approach likely is the most time-consuming and requires a lot of coordination.\n\n\n(b) Sources to address E1 Standard:\n\nE1. Sub topics:\n\nClimate Change Mitigation\nClimate Change Adaptation\nEnergy\n\nThe firm shall consider the list in ESRS I AR.16 (2023 ESRS) or ESRS I Appendix A (2025 ESRS). When performing its materiality assessment, the undertaking shall consider this list of sustainability matters covered in the topical ESRS. When, as a result of the undertaking’s materiality assessment (see ESRS II IRO-1), a given sustainability matter in this list is assessed to be material, the undertaking shall report according to the corresponding Disclosure Requirements of the relevant topical ESRS.\nE1.Climate Change Mitigation\n\nQ. In how far do the firm’s (or its value chain’s) actions contribute (positively/negatively) to climate change?\n\nNegative impact: Sale of internal combustion engine (ICE) vehicles contributes significantly to downstream Scope 3 greenhouse gas emissions from vehicle use.\nNegative impact: Energy-intensive workshop, paint shop, and dealership operations generate operational (Scope 1 and 2) emissions.\nPositive impact: Sale of electric vehicles (EVs) supports the reduction of transport-related GHG emissions.\nPositive impact: Installation and operation of EV charging infrastructure enable the transition to low-emission mobility in the wider economy.\n\nQ. In how far is the firm (or ist value chain) affected by the transition to a low-carbon economy, considering potential financial benefits or risks?\n\nTransition risk: Decreasing demand and residual values for ICE vehicles due to stricter climate regulation and changing customer preferences.\nRegulatory risk: Increased costs from tighter emissions regulations, reporting obligations, and carbon pricing affecting operations and fleet services.\nMarket risk: Potential loss of competitiveness if EV offerings, charging services, or low-carbon services lag behind competitors.\nMarket opportunity: Growing demand for EVs can increase revenues from vehicle sales and related services.\nBusiness model opportunity: Expansion of EV charging infrastructure and e-mobility services for private and fleet customers.\nFinancial opportunity: Access to green financing and sustainability-linked funding due to alignment with decarbonization objectives.\nEfficiency opportunity: Energy efficiency measures in dealerships and workshops can reduce operating costs and emissions simultaneously.\n\n\nE1.Climate Change Adaptation\n\nQ. In how far does the firm (or its value chain) adapt to actual/potential impacts of climate change?\n\nNegative impact: Limited and uncoordinated adaptation measures at dealership sites may increase the vulnerability of employees, customers, and local communities during extreme heat, storms, or flooding events.\nPositive impact: Existing basic resilience measures (e.g., climate-controlled workshops, stormwater management, asset protection) contribute to reduced climate-related damage and operational disruption.\nValue chain impact: Disruptions at logistics partners, vehicle manufacturers, or fleet customers due to extreme weather can indirectly affect service continuity and customer reliability.\n\nQ. In how far is the firm (or ist value chain) affected by climate change effects, considering potential financial benefits or risks?\n\nPhysical risk: Heatwaves may reduce employee productivity and increase health and safety risks in workshops and outdoor operations.\nPhysical risk: Flooding, storms, or heavy rainfall may damage dealership buildings, vehicle stock, charging infrastructure, and IT systems, leading to financial losses.\nOperational risk: Climate-related disruptions at suppliers and transport partners may delay vehicle deliveries, spare parts availability, and fleet services.\nFinancial risk: Rising insurance premiums or reduced insurability due to increased physical climate risks at exposed locations.\nMarket opportunity: Growing demand for climate-resilient vehicles, fleet solutions, and related services due to more frequent extreme weather events.\nOperational opportunity: Proactive investment in climate-resilient infrastructure reduces downtime, repair costs, and business interruption losses.\nService opportunity: Expansion of advisory and fleet management services focused on climate resilience and operational continuity.\nReputational opportunity: Positioning as a reliable and resilient mobility partner for corporate and fleet customers under changing climate conditions.\n\n\nE1.Energy\n\nQ. In how far do the firm’s (or its value chain’s) actions contribute (positively/negatively) to the consumption of energy?\n\nNegative impact: High electricity and gas consumption from dealership buildings, workshops, paint shops, lighting, heating, IT systems, and charging infrastructure increases overall energy demand.\nNegative impact (value chain): Energy use associated with vehicle preparation, logistics, and outsourced service providers adds to indirect energy consumption.\nPositive impact: Implementation of energy-efficient equipment, building retrofits, and process optimization can reduce energy demand and related environmental pressures.\nPositive impact: Potential on-site renewable energy generation (e.g., rooftop PV) can reduce reliance on external energy supply.\n\nQ. In how far is the firm (or ist value chain) affected by its use of energy, considering potential financial benefits or risks?\n\nCost risk: Exposure to volatile electricity and gas prices increases operating costs and reduces profit margins.\nRegulatory risk: Stricter energy efficiency standards and reporting requirements may lead to additional compliance and investment costs.\nOperational risk: Energy supply disruptions or grid constraints may affect workshop operations, IT systems, and EV charging availability.\nTransition-related risk: Increased pass-through of carbon costs in energy prices may further elevate energy expenses.\nEfficiency opportunity: Energy efficiency measures (LED lighting, efficient compressors, heat recovery, smart building systems) reduce long-term operating costs.\nSelf-generation opportunity: Investment in on-site renewable energy (e.g., photovoltaic systems) lowers energy procurement costs and exposure to price volatility.\nFlexibility opportunity: Smart charging and load management for EV infrastructure enable demand-side flexibility and potential grid service revenues.\nReputational opportunity: Demonstrating low and efficient energy use can strengthen the company’s sustainability profile with customers, OEM partners, and financiers.\n\n\n\n\n\n\n\nTable 2. IRO Database."
  },
  {
    "objectID": "whiteboard03/POST/project_3/index.html#c.-phase-3-iro-assessment",
    "href": "whiteboard03/POST/project_3/index.html#c.-phase-3-iro-assessment",
    "title": "Double Materiality Assessment",
    "section": "[C]. Phase 3 – IRO Assessment",
    "text": "[C]. Phase 3 – IRO Assessment\nEquipped with a list of relevant IROs, let’s set up to assess their materiality.\n\nSeverity: Size of environmental effect (harm).\nExtent: Size of financial effect (money).\nScope(moral dimension): How many people / How much environment is affected.\nIrremediability(moral dimension): How reversible the harm is.\n\\(P(\\text{Occurance}_i)\\): probability the impact/risk/opportunity materialises.\n(a) formulas used to calculate Impact Materiality scores (for Impact): \\[\n\\begin{aligned}\n\\text{SCORE}_{I} = \\text{Severity}_i &\\times P(\\text{Occurance}_i)  \\quad \\text{with   }P(\\text{Occurance}_i)=1 \\; \\text{always for human right, } \\\\\n& \\text{where }\\; \\text{Severity}_i = \\begin{cases}\n\\dfrac{1}{2}\\bigl(\\text{Extent}_i + \\text{Scope}_i\\bigr),\n& \\text{if impact } i \\text{ is positive}, \\\\[8pt]\n\\dfrac{1}{3}\\bigl(\\text{Extent}_i + \\text{Scope}_i + \\text{Irremediability}_i\\bigr),\n& \\text{if impact } i \\text{ is negative}.\n\\end{cases}\n\\end{aligned}\n\\]\n(b) formulas used to calculate Financial Materiality scores (for Risk/Opportunity): \\[\n\\begin{aligned}\n\\text{SCORE}_{R\n/O} = \\text{Extent}_i &\\times P(\\text{Occurance}_i)   \\\\\n& \\text{where }\\; \\text{Extent}_i = \\begin{cases}\n\\frac{\\text{€ impact}}{\\text{firm size}},\n& \\text{ how much money is at stake / how big the firm is}... \\\\[8pt]\n\\frac{\\text{E[Financial impact]}}{\\text{Reference Financial KPI}}\n& \\text{}\n\\end{cases}\n\\end{aligned}\n\\] Why Extent is expressed as a ratio? A certain loss can be huge for a small firm, and tiny for a large firm. So we scale the impact.\n\nnumerator: E[Financial impact] includes:\n\nExpected revenue loss\nCost increase\nAsset impairment\nRegulatory fines\n\ndenominator: Reference Financial KPI (the firm’s relevant financial scale). Different risks hit different parts of the financial statements, so the denominator must match the economic channel of the risk. This includes:\n\nRevenue\nOperating profit\nAsset values\nEquity (shareholders’ capital)\n\n\n\n\n\nHow exactly is the threshold value - ranging from Extent: 1 to Extent: 5 - for risks/opportunities determined? it is sensible to adjust the thresholds according to the specific industry of the firm to adjust risks to the economic conditions of the business. In general, the following procedure has proved successful:\n\nIf risk management system exists: Use of the existing monetary threshold (80%, significance level of 0.2) for consideration as an anchor for medium (SCORE: 3)\nRevenue 0.5% ~ 3.0% for medium (SCORE: 3)\nOperating profit 3% ~ 10% for medium (SCORE: 3)\n\n\nAgain, if possible, try to orientate yourself on existing risk management methods.\nSo which topical standard is material? - In Extent, Scope, Irremediability, if any of them reaches the maximum, it becomes automatically material. - In Extent, Scope, Irremediability, if any of them imples the human rights, it becomes automatically material. - Finally, An overall SCORE above 2.5 is material."
  },
  {
    "objectID": "whiteboard03/POST/project_3/index.html#d.-phase-4-reporting-data",
    "href": "whiteboard03/POST/project_3/index.html#d.-phase-4-reporting-data",
    "title": "Double Materiality Assessment",
    "section": "[D]. Phase 4 – Reporting Data",
    "text": "[D]. Phase 4 – Reporting Data\nAfter completing the identification and assessment of IROs, we prepare the data for reporting. In general, the firm is required to disclose as below:\n\nIn the final reporting step, the undertaking must disclose how the materiality assessment was carried out and what its outcomes are. This includes:\n\nA description of the process to identify and assess material impacts, risks and opportunities (ESRS II IRO-1),\nthe material IROs identified and how they relate to the company’s strategy and business model (ESRS II SBM-3),\nA clear indication of which topical ESRS disclosure requirements are covered in the sustainability statement (ESRS II IRO-2).\nThe undertaking must also explain how material information was determined, including the thresholds and criteria applied.\n\nIn addition, the company must report how its administrative, management and supervisory bodies (AMB) are informed about and oversee material sustainability matters. This includes how the AMB considers material IROs in the company’s strategy and risk management and how these matters were addressed during the reporting period."
  },
  {
    "objectID": "whiteboard03/POST/project_3/index.html#bibliography",
    "href": "whiteboard03/POST/project_3/index.html#bibliography",
    "title": "Double Materiality Assessment",
    "section": "Bibliography",
    "text": "Bibliography\n[1] EFRAG: Implementation Guidance 1 – Materiality Assessment (MAIG) - https://www.efrag.org/sites/default/files/sites/webpublishing/SiteAssets/IG%201%20Materiality%20Assessment_final.pdf."
  },
  {
    "objectID": "whiteboard03/POST/project_1/index.html",
    "href": "whiteboard03/POST/project_1/index.html",
    "title": "EU-Sustainability Reporting - E1",
    "section": "",
    "text": "Be transparent: How much GREEN are you?\nESRS (EU Sustainability Reporting Standards) developed by EFRAG (EU Financial Reporting Advisory Group) provides mandatory reporting standards under the CSRD (Corporate Sustainability Reporting Directive) in the EU. ESRS define how companies must report on sustainability-related impacts, risks, and opportunities. They include two cross-cutting standards (ESRS 1 and ESRS 2) that explain the general principles, reporting structure, and required company-wide disclosures. The environmental standards (E1–E5) cover climate change, pollution, water and marine resources, biodiversity, and resource use and circular economy. The social standards (S1–S4) focus on a company’s own workforce, workers in the value chain, affected communities, and consumers. Finally, the governance standards (G1) addresses ethical business conduct, including corruption, lobbying, and transparency. ESRS operationalises DMA to assess both inward financial risk and outward environmental effect."
  },
  {
    "objectID": "whiteboard03/POST/project_1/index.html#a.-emission-calculation",
    "href": "whiteboard03/POST/project_1/index.html#a.-emission-calculation",
    "title": "EU-Sustainability Reporting - E1",
    "section": "[A]. Emission Calculation",
    "text": "[A]. Emission Calculation\nFor each of the firm’s activity described in Table 1, let me explain which scopes of CO2 emissions are being addressed. Let’s calculate the amount of CO2 emissions associated with the selected scope in each situation.\n\n\n\nTable 1. Key production information of Kinove AG\n\n\n\n\n\nScope 1 emissions are the direct greenhouse gas emissions from sources a company owns or controls, such as its factories and vehicles. Scope 2 emissions are indirect emissions from the generation of purchased energy, such as electricity, heating, or cooling, and are reported separately because they are closely linked to a company’s energy use. Scope 3 emissions include all other indirect emissions along the value chain and are divided into upstream emissions (e.g., raw materials, suppliers, and transportation) and downstream emissions (e.g., product use, distribution, and disposal), typically making up the largest share of a company’s total footprint.\n\n\n\n\n\n\n\n\nNote\n\n\n\nactivity 1 ⇒ {Scope 1}: Due to the company paid fuel card (controlled by the firm), a car usage as transportation is considered as a direct emission. Therefore, EMISSION = 5.7 tons in Scope 1. Although private car use (might be Scope 3) might need to be excluded from Scope 1, 95% ((227,760 – 11,388) / 227,760) were already associated with the business, thus for the sake of simplicity, we assign 100% of emission to Scope 1 while disclosing that car is also used privately.\nactivity 2 ⇒ {Scope 1 + 3}: Own freight truck fleet is a direct scope 1 emission while leased vehicles is scope 3. Therefore, \\[\n\\begin{aligned}\nEMISSION &= 0.25 \\times 25 \\times 223 \\; \\text{tons} = 1,393.75 \\; \\text{tons (in Scope 3)} \\\\\nEMISSION &= 0.75 \\times 25 \\times 223 \\; \\text{tons} = 4,181.25 \\; \\text{tons (in Scope 1)}\n\\end{aligned}\n\\]\nactivity 3 ⇒ {Scope 2}: They are indirect emissions from electricity. Therefore,\n\\[\nEMISSION = 70,000\\;\\text{kWh} \\times 0.0004 \\; \\text{tons/kWh} + 1,200 \\; \\text{tons} = 1,228 \\; \\text{tons (in Scope 2)}\n\\]\nactivity 4 ⇒ {Scope 3}: The CFO used a business jet (a chartered jet not under direct operational control), while four delegates and one intern used commercial flights. In response, the company attempted to offset the emissions by paying for the planting of 1,000 trees at €2.99 per tree, each covering 0.1 tons of emissions. However, the compensation is recorded as zero, because total emissions must be reported in gross terms under ESRS E1, AR 20(d)(Gross reporting means emissions must be disclosed before any offsets or compensation are deducted.). Based on the following, the total emission is EMISSION = 51.27 tons in Scope 3 (oneway).\n\\[\n\\begin{aligned}\nEMISSION_{CFO} &= 9,178 \\;\\text{km} \\div 750 \\; \\text{km/h} \\times 2 \\; \\text{tons/h} = 24.47 \\; \\text{tons (in Scope 3)} \\\\\nEMISSION_{del} &= 4 \\times 5.9 \\; \\text{tons/h} = 23.6  \\; \\text{tons (in Scope 3)} \\\\\nEMISSION_{int} &= 1 \\times 3.2 \\; \\text{tons/h} = 3.2  \\; \\text{tons (in Scope 3)}\n\\end{aligned}\n\\]\nactivity 5 ⇒ {Scope 3}: They are indirect emissions from employee commutes. Therefore,\n\\[\n\\begin{aligned}\nEMISSION_{old} &= 1,500 \\times 20 \\;\\text{days} \\times 12 \\;\\text{months} \\times \\text{x} \\; \\text{tons} = 25,000 \\; \\text{tons (in Scope 3); x=5/72} \\\\\nEMISSION_{a} &= 750 \\times 16 \\;\\text{days} \\times 12 \\;\\text{months} \\times 5/72 \\; \\text{tons} = 10,000 \\; \\text{tons (in Scope 3)} \\\\\nEMISSION_{b} &= 750 \\times 20 \\;\\text{days} \\times 12 \\;\\text{months} \\times 5/72 \\; \\text{tons} = 12,500 \\; \\text{tons (in Scope 3)}\n\\end{aligned}\n\\]\nactivity 6 ⇒ {Scope 3}: They are indirect emissions from waste management. However, as the firm donates the items, the indirect emissions from end of life waste management (for waste generated in operations, Draft ESRS E1 AR 26 “Waste generated in operations”) are not considered. The waste is no longer the company’s waste once it is donated. Here, Scope 3 would only consider direct waste or downstream waste through sold products. Therefore, EMISSION = 0 tons in Scope 3."
  },
  {
    "objectID": "whiteboard03/POST/project_1/index.html#b.-developing-disclosure-requirements",
    "href": "whiteboard03/POST/project_1/index.html#b.-developing-disclosure-requirements",
    "title": "EU-Sustainability Reporting - E1",
    "section": "[B]. Developing Disclosure Requirements",
    "text": "[B]. Developing Disclosure Requirements\nNow, let’s prepare the disclosure requirements of ESRS E1 as far as possible considering the information provided above. Whenever information is not available, we state what the firm shall provide in terms of disclosures.\n\n\n\nFigure 2. ESRS Disclosure Requirements E1-1 to E1-9 (main requirements, November 2025 draft)\n\n\n\n\n\n\n\n\nNote\n\n\n\nDisclosure Requirement E1-1 – Transition plan for climate change mitigation:\n\nWe can describe the entity’s transition plan to reduce carbon impact:\n\nTarget to reduce emissions by 50% by the year 2035.\nKey actions include replacing 20% of its truck fleet with battery-powered vehicles\nPlan adoption by executives and supervisory board\n\n\nDisclosure Requirement E1-2 – Identification of climate-related risks and scenario analysis:\n\nNo further information available here, but we can describe ways to access short-term, medium-term, long-term business activities in light of climate related risks, differentiating the risk into physical or transition risk…and methodologies? scenario analysis?\n\nDisclosure Requirement E1-3 – Resilience in relation to climate change:\n\nNo further information available here, but we can describe the resilience of the firm’s strategy and business model to climate-related risks.\n\ninclude the results of a climate resilience analysis\ninclude significant areas of uncertainty in the assessment\ninclude business model to climate change over short, medium and long term\n\n\nDisclosure Requirement E1-4 – Policies related to climate change mitigation and adaptation:\n\nNo further information available here, but we can describe policies that address climate change mitigation and adaptation IRO.\n\nDisclosure Requirement E1-5 – Actions and resources in relation to climate change mitigation and adaptation:\n\nWe can describe key actions to address climate change mitigation and adaptation.\n\npurchasing EV to replace the old truck fleet.\nAlready achieved 10% reduction, so we expect another 10% next year\n\n\nDisclosure Requirement E1-6 – Targets related to climate change:\n\nWe can describe targets related to climate change mitigation and adaptation.\n\nTarget of 50 % reduction by 2035 relative to base year (2016)\n\n\nDisclosure Requirement E1-7 – Energy consumption and mix:\n\nInformation is available only partially, but we can describe energy consumption and composition of energy sources consumed.\n\nDisclosure Requirement E1-8 – Gross Scope 1,2,3 GHG emissions:\n\nTotal Emission: 200,000.28 tons + 48,606.25 tons + 139,445.02 tons = 388051,55 tons\n\nScope 1: 195,813.33 tons + 4,181.25 tons + 5.7 tons = 200,000.28 tons\nScope 2: 47,378.25 tons + 1,228 tons = 48,606.25 tons\nScope 3: 115,448.73 tons + 23,996.29 tons = 139,445.02 tons\n\n\nDisclosure Requirement E1-9 – GHG removals and GHG mitigation projects financed through carbon credits:\n\nNo actions were undertaken during the reporting year.\n\n\n\nNote that, according to ESRS, transition plans show the relationship between firms’ sustainability activities, climate goals and corporate strategies. Policies refer to tactics within the firm to address specific sustainability matters covered by the transition plan."
  },
  {
    "objectID": "whiteboard03/POST/project_1/index.html#bibliography",
    "href": "whiteboard03/POST/project_1/index.html#bibliography",
    "title": "EU-Sustainability Reporting - E1",
    "section": "Bibliography",
    "text": "Bibliography\n[1] Corporate Sustainability Reporting Directive: Directive (EU) 2022/2464 of December 14, 2022. Available at https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A02022L2464-20250417 (Amended April 2025).\n[2] ESRS E1: Climate change. Available at https://www.efrag.org/sites/default/files/media/document/2025-12/November_2025_ESRS_E1.pdf (November 2023)."
  },
  {
    "objectID": "whiteboard02/POST/project_3/index.html",
    "href": "whiteboard02/POST/project_3/index.html",
    "title": "Quality Monitoring for Casting RUL Analysis",
    "section": "",
    "text": "During continuous casting machine (CCM) operation, the main production issue that comes up during the operation of sleeves is that defects appear on the surface of the copper pipe of the sleeve and distort the profile of its inner cavity.\nThe automated process control system records casting parameters into a production database. The collected data represent averaged process parameters across all strands within each cast, with the primary strand-level variation captured through mould sleeve resistance.\nFollowing mould sleeve removal and inspection, process parameters, ingot geometry, and related operational attribute data are extracted from the SCADA system. Although sourced from a real production environment, the data were subsequently cleaned, aggregated, and pre-processed to support Remaining Useful Life (RUL) modeling.\nThe RUL variable in the dataset is derived from the resistance (tonn) (sleeve failure) variable by computing for each sleeve, “crystallizer”, “casting stream”, the difference between the highest observed resistance value and the current resistance value.\nThe expected Useful Life in tons should be as follows:\nWe might want to predict the remaining time the sleeve can operate before failing, using sensor data and historical patterns to enable predictive maintenance.\nThe dataset uses:\n\\[\nRUL = \\mathbb{max}( resistance ) - current \\; resistance\n\\] So it is a declining time-to-failure indicator which is a standard in supervised RUL estimation datasets. Time-aligned degradation data (like resistance over time) is used to estimate RUL via survival models.\nThe project’s goal is to ensure the quality of data collected during the operation of the Continuous Casting Machine (CCM). That data includes sleeve resistance, strand identifiers, and up to 15 process parameters.\nHere, I’m laying the foundation: collecting reliable, validated, complete, fresh, and interpretable data that can power technical diagnostics and predictive maintenance.\nI designed and implemented a scalable data warehouse on BigQuery using a tiered Bronze–Silver–Gold structure inspired by Medallion Architecture. Data ingestion was handled through Kafka, while transformations and modeling were managed with dbt to ensure modular, version-controlled pipelines.\nFor orchestration, I used Airflow, enabling automated execution, validation, and alerting to maintain high pipeline reliability. This layered approach allowed us to clearly separate raw, cleaned, and analytics-ready data, improving data governance, transparency, and access control across the system.\nAs a result, our BI dashboards will be consistently refreshed with accurate, high-quality data, supporting timely and reliable decision-making. The system also introduced robust monitoring and clear data lineage, with ongoing efforts focused on reducing latency and further optimizing observability."
  },
  {
    "objectID": "whiteboard02/POST/project_3/index.html#product-requirements-document-prd",
    "href": "whiteboard02/POST/project_3/index.html#product-requirements-document-prd",
    "title": "Quality Monitoring for Casting RUL Analysis",
    "section": "Product Requirements Document (PRD)",
    "text": "Product Requirements Document (PRD)\n\n[A]. Overview\n\n\nbackground\n\n\nobjective\n\n\n\n\n\n\n\n\n\n\n\n\nID\nFeature\nTest Type\nDescription\n\n\n\n\nT01\nRUL accuracy\nRegression test\nCompare model output vs. true RUL on historical data\n\n\nT02\nFeature completeness\nNot Null\nEnsure no nulls in casting parameter inputs\n\n\nT03\nResistance validity\nRange check\nEnsure resistance is within known tonnage range\n\n\nT04\nAlert precision\nSLA test\nAlert only when RUL &lt; threshold with high accuracy\n\n\nT05\nCasting temperature validity\nValue domain\nValidate steel_temperature is within 1400–1600°C"
  },
  {
    "objectID": "whiteboard02/POST/project_3/index.html#data-acquisition",
    "href": "whiteboard02/POST/project_3/index.html#data-acquisition",
    "title": "Quality Monitoring for Casting RUL Analysis",
    "section": "Data Acquisition",
    "text": "Data Acquisition\nIn the course of CCM operation, the automatic control system that runs the process of casting ingots creates a database of casting parameters. The collected parameters are averaged data for all the strands in each cast; the only thing that differs is the resistance of the sleeve for each strand. After removing the mould sleeve for inspection, the initial data on the process parameters of casting, the geometry of obtained ingots and other attributes can be uploaded from the SCADA. The data were collected from a real production facility but after that they were processed, cleared, aggregated and prepared to solve the RUL problem."
  },
  {
    "objectID": "whiteboard02/POST/project_3/index.html#data-management-with-medallion-architecture",
    "href": "whiteboard02/POST/project_3/index.html#data-management-with-medallion-architecture",
    "title": "Quality Monitoring for Casting RUL Analysis",
    "section": "Data Management with Medallion Architecture",
    "text": "Data Management with Medallion Architecture\nThe following dataset represents a Silver-layer operational view within a medallion architecture - Bronze, Silver, Gold - for a continuous casting process investigation. Raw sensor streams and process logs are first ingested into the Bronze layer as immutable records. These raw signals are then cleaned, standardized, time-aligned, and enriched with production metadata, cooling parameters, chemical composition, and equipment identifiers to form this Silver dataset.\nEach row corresponds to a casting event or production segment, and the Remaining Useful Life (RUL) variable serves as the analytical target. By structuring the data at the Silver layer, we enable reliable linkage between production characteristics: production info, process parameters: casting process parameters, cooling & crystallizer behavior: Water Consumption, residue info, chemical composition of steel: material properties, chemical composition, operational identifiers: time identification, equipment context, and target variable: RUL, which can then be aggregated or modeled in the Gold layer to produce RUL predictions and maintenance KPIs. This layered design ensures data quality and auditability while supporting predictive maintenance analytics."
  },
  {
    "objectID": "whiteboard02/POST/project_1/index.html",
    "href": "whiteboard02/POST/project_1/index.html",
    "title": "Data Pipelining with AWS",
    "section": "",
    "text": "Data Pipeline Architecture & Roadmap.\nIn this project, we introduce a cloud-based data pipelining using AWS. We collect “Korean housing price” data from a public API (https://apis.data.go.kr/1613000/RTMSDataSvcAptTrade/getRTMSDataSvcAptTrade?LAWD_CD=11110&DEAL_YMD=202407&serviceKey=…..) and store the raw data in an ‘S3’ data lake storage. Using Python scripts running on AWS ‘EC2’ (computing resource), we process the data — for example, deriving building prices — and send the results back to ‘S3’.\nThe pipeline then performs ETL operations using AWS ‘Lambda’ for lightweight tasks and AWS ‘Glue’ for larger-scale processing. The cleaned and transformed data is stored in a separate ‘S3’ bucket. We register the processed data in the AWS ‘Glue Data Catalog’ so it can be queried directly with AWS ‘Athena’ using SQL.\nIn parallel, we load a structured version of the data into AWS ‘Redshift’ as our data warehouse to support downstream analytics. For visualization and reporting, we use AWS ‘QuickSight’ to connect to ‘Redshift’ and generate dashboards and insights.\nOnce this pipeline is in place, it can easily be extended — for example, by adding new API sources or additional transformation steps. While the current pipeline is not yet fully automated, we plan to orchestrate the workflows using Apache Airflow so it can run on a daily, hourly, or monthly basis."
  },
  {
    "objectID": "whiteboard02/POST/project_1/index.html#step-01.-data-collection",
    "href": "whiteboard02/POST/project_1/index.html#step-01.-data-collection",
    "title": "Data Pipelining with AWS",
    "section": "Step 01. Data Collection",
    "text": "Step 01. Data Collection\nFirst, we develop an API integration to retrieve real-time housing price data using a secured API key. The API data ingestion can be facilitated using the software “Postman.com” for testing and validation. Once the HTTP requests return the data, we collect and store the raw responses in AWS S3 data lake for further processing.\n\n\n\nAPI Scraping Examples: fetching XML data using API Call with key-based authentication - The sample result shows 10 records in each page and 60 items in total.\n\n\nIn AWS, we first create an S3 bucket and a designated folder to store the raw data. The following Python script is used to retrieve data from the API and perform initial processing. By “processing the raw data,” it mainly mean extracting the required columns from the XML response and uploading the collected dataset to the S3 data lake.\n\n\nCode\n#######################################################################################\n# function 00\ndef processing_datas(root):\n    \"\"\"\n        data structure\n        [{transaction #1}, {transaction #2}]\n    \"\"\"\n    list_temp = list()\n\n    for item in root.iter(\"item\"):\n        dict_item = {\n            \"sggCd\": item.find(\"sggCd\").text,\n            \"umdNm\": item.find(\"umdNm\").text,\n            \"aptNm\": item.find(\"aptNm\").text,\n            \"jibun\": item.find(\"jibun\").text,\n            \"excluUseAr\": item.find(\"excluUseAr\").text,\n            \"dealYear\": item.find(\"dealYear\").text,\n            \"dealMonth\": item.find(\"dealMonth\").text,\n            \"dealDay\": item.find(\"dealDay\").text,\n            \"dealAmount\": item.find(\"dealAmount\").text,\n            \"floor\": item.find(\"floor\").text,\n            \"buildYear\": item.find(\"buildYear\").text,\n            \"cdealType\": item.find(\"cdealType\").text,\n            \"cdealDay\": item.find(\"cdealDay\").text,\n            \"dealingGbn\": item.find(\"dealingGbn\").text,\n            \"estateAgentSggNm\": item.find(\"estateAgentSggNm\").text,\n            \"rgstDate\": item.find(\"rgstDate\").text,\n            \"aptDong\": item.find(\"aptDong\").text,\n            \"slerGbn\": item.find(\"slerGbn\").text,\n            \"buyerGbn\": item.find(\"buyerGbn\").text,\n            \"landLeaseholdGbn\": item.find(\"landLeaseholdGbn\").text\n        }\n        list_temp.append(dict_item)\n\n    return list_temp\n\n\n#: pip install requests \n#######################################################################################\n# function 01\ndef get_apt_trade_from_api(service_key: str, lawd_cd: str, deal_ymd: str):\n    import xml.etree.ElementTree as ET\n    import requests\n\n    list_result = list()\n    num_of_rows = 50\n    page_no = 1\n\n\n    while True:\n        end_point_url = (\"http://apis.data.go.kr/1613000/RTMSDataSvcAptTrade/getRTMSDataSvcAptTrade?\"\n                         f\"serviceKey={service_key}&LAWD_CD={lawd_cd}&DEAL_YMD={deal_ymd}\"\n                         f\"&numOfRows={num_of_rows}&pageNo={page_no}\")\n\n        response = requests.get(end_point_url)\n        root = ET.fromstring(response.text)\n        total_count = int(root.find(\"body/totalCount\").text)\n\n        # 2. data processing\n        list_result += processing_datas(root)\n\n        # Pagination\n        if len(list_result) &gt;= total_count:\n            break\n\n        page_no += 1\n\n    return list_result\n\n\n#: pip install jsonlines\n#######################################################################################\n# function 02\ndef save_file(content, file_name, file_type):\n    \"\"\"\n        data structure\n        { transaction #1 info }\n        { transaction #2 info }\n        ...\n        { transaction #n info }\n    \"\"\"\n    import jsonlines\n\n    if file_type == \"json\":\n        with jsonlines.open(file_name, \"w\") as f:\n            f.write_all(content)\n\n\n#: pip install boto3\n#######################################################################################\n# function 03\ndef upload_to_s3(file_name, bucket_name, object_name):\n    from botocore.exceptions import NoCredentialsError\n    import boto3\n\n    s3_client = boto3.client(\"s3\")\n\n    try:\n        s3_client.upload_file(file_name, bucket_name, object_name)\n    except NoCredentialsError:\n        print(\"Cannot found AWS permission.\")\n\n\n\n#################################################################################################\n#################################################################################################\n#: pip install python-dotenv \n#&gt; Notice: create \".env\" file in advance and save our SERVICE_KEY from the API (for security reason..) \n#################################################################################################\n#################################################################################################\ndef main():\n    # prepare to bring your SERVICE_KEY\n    from dotenv import load_dotenv\n    import os\n\n    # set the variable for SERVICE_KEY\n    load_dotenv()\n    # bring ur service key\n    service_key = os.getenv(\"SERVICE_KEY\")\n\n    # output formatting\n    file_type = \"json\"\n    bucket_name = \"khousepricing\"\n\n    # bring essential \"key\" and \"value\" as listed in the API document\n    list_LAWD_CD = [\n        \"11110\"  # AREA_1\n        , \"11140\"  # AREA_2\n        , \"11170\"  # AREA_3\n        , \"11200\"  # AREA_4\n        , \"11215\"  # AREA_5\n        , \"11230\"  # AREA_6\n        , \"11260\"  # AREA_7\n        , \"11290\"  # AREA_8\n        , \"11305\"  # AREA_9\n        , \"11320\"  # AREA_10\n        , \"11350\"  # AREA_11\n        , \"11380\"  # AREA_12\n        , \"11410\"  # AREA_13\n        , \"11440\"  # AREA_14\n        , \"11470\"  # AREA_15\n        , \"11500\"  # AREA_16\n        , \"11530\"  # AREA_17\n        , \"11545\"  # AREA_18\n        , \"11560\"  # AREA_19\n        , \"11590\"  # AREA_20\n        , \"11620\"  # AREA_21\n        , \"11650\"  # AREA_22\n        , \"11680\"  # AREA_23\n        , \"11710\"  # AREA_24\n        , \"11740\"  # AREA_25\n    ]\n\n    for DEAL_YMD in [\"202401\"]:\n        for LAWD_CD in list_LAWD_CD:\n            file_name = f\"{DEAL_YMD}_{LAWD_CD}_result.json\"\n            object_name = f\"apt-trade-raw/DEAL_YMD={DEAL_YMD}/LAWD_CD={LAWD_CD}/result.json\"\n\n            # 1. fetch raw data from API\n            trade_result = get_apt_trade_from_api(service_key, LAWD_CD, DEAL_YMD)\n\n            # 3. data processing\n            save_file(trade_result, file_name, file_type)\n\n            # 4. save it as a file and store it in S3 datalake\n            upload_to_s3(file_name, bucket_name, object_name)\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\nAPI Scraping Results: raw data DEAL_YMD=202401/ delieverd by code “mycode.py”\n\n\nNote that, we create new file and name it as .env to store sensitive service keys as shown below that should not be hard-coded in the script.\n\nSERVICE_KEY= xxxxxxx\n\nThis is retrieved by service_key = os.getenv(“SERVICE_KEY”) in the code."
  },
  {
    "objectID": "whiteboard02/POST/project_1/index.html#step-02.-ec2-instance",
    "href": "whiteboard02/POST/project_1/index.html#step-02.-ec2-instance",
    "title": "Data Pipelining with AWS",
    "section": "Step 02. EC2 instance",
    "text": "Step 02. EC2 instance\nDEAL_YMD=202401/, DEAL_YMD=202402/, DEAL_YMD=202403/,… Now we’re running the code on the AWS EC2 instance. To launch the instance, we selected:\n\nOperating System: Ubuntu 24.04 LTS (HVM)\nInstance Type: t3.small (2 vCPUs, 2 GiB memory)\nKey Pair: Created to enable secure SSH access\n\nSince we are using a Windows laptop, we generated the key pair in “.ppk” format to use PuTTY. After completing the setup, we can see the new instance running in the AWS Management Console.\n\n\n\nEC2 instance in the AWS Management Console\n\n\nTo access the instance, first retrieve its public IP address and establish a connection between our local device and the EC2 instance on the AWS remote server (using PuTTY). Before proceeding, we verify that the instance’s security group allows inbound SSH traffic. If SSH is not enabled, we add a new inbound rule to allow “SSH access” to the server. Because our local device runs Windows, we use PuTTY to establish the connection.\nNext, we open a new PUTTY terminal window specifically for transferring all necessary files in the local device to the AWS EC2 instance using the pscp command, while keeping the PUTTY terminal connected to the AWS server.\nFor PuTTY users on Windows, 1)Open PuTTY. 2)Go to Session and Enter the Host Name as “public IP” ubuntu@16.170.170.76 for example. Finally, go to Connection to SSH and in Auth: Browse and select key file:“.ppk” that was offered by AWS EC2 instance.\nOn the EC2 instance, we will set up a Python 3 environment. To prepare for this, go back to our local terminal and first generate a requirements.txt file that contains all Python packages required to run the script mycode.py.\n\npip freeze &gt; requirements.txt\n\nThe following is to securely transfer the necessary files from the local machine to the EC2 instance (PuTTY):\n\npscp -i MyKeypairForEC2.ppk requirements.txt ubuntu@16.170.170.76:/home/ubuntu/\npscp -i MyKeypairForEC2.ppk .env ubuntu@16.170.170.76:/home/ubuntu/\npscp -i MyKeypairForEC2.ppk mycode.py ubuntu@16.170.170.76:/home/ubuntu/\n\n\n\n\nnew PuTTY terminal in local machine\n\n\nNote that in the example above: ubuntu@172.31.7.42 = house number inside the gated community (Private IP of EC2 instance) and ubuntu@16.170.170.76 = street address that the whole world can use to find you (Public IP of AWS)\nIn PUTTY terminal (using EC2 instance), we run “mycode.py” to store the raw data into the S3 data lake.\n\nCheck what’s inside: ls -lrt\ninstall: sudo apt install python3-pip\nupdate: sudo apt update\ncreate a virtual ENV 1: python3 -m venv ~/.venvs/myproj\ncreate a virtual ENV 2: . ~/.venvs/myproj/bin/activate\ninstall: pip3 install -r requirements.txt\ninstall: curl “https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip” -o “awscliv2.zip”\ninstall: unzip awscliv2.zip\ninstall: sudo ./aws/install\nconfigure on EC2: aws configure\n\nAWS Access Key ID: \nAWS Secret Access Key: \nDefault region: eu-north-1\nDefault output format: json\n\nRUN: python mycode.py\n\nNow, return to the S3 bucket and verify that the raw data has been successfully uploaded.\n\n\n\nRaw (JSON) data stored in S3 data lake using the file “mycode.py”"
  },
  {
    "objectID": "whiteboard02/POST/project_1/index.html#step-03.-etl",
    "href": "whiteboard02/POST/project_1/index.html#step-03.-etl",
    "title": "Data Pipelining with AWS",
    "section": "Step 03. ETL",
    "text": "Step 03. ETL\nWhen we want to process raw data stored in S3 (or other sources), clean it, transform it, and load it somewhere else (like S3, RDS, Redshift, DynamoDB), AWS gives us two options.\n\nAWS Lambda: Good for lightweight, event-driven ETL\nAWS Glue: Managed ETL service for larger workloads\n\nIn this project, we focus on executing large-scale ETL using AWS Glue. As the initial step, we build a Glue Data Catalog by running a Glue Crawler, which automatically infers table schemas from the raw data. To configure the Crawler, we add the data source by connecting to the S3 bucket through the AWS Console. The Crawler then scans the data, identifies the structure, and creates the corresponding table schemas and partitions.\nA Glue Data Catalog is ready for efficient processing of large-scale datasets.\n\n\n\nCrawler created a Data Catalog\n\n\n\n\n\nRaw tables drawn from a Data Catalog\n\n\nIn short,\n\nSource (to fetch): S3\nTransform: SQL queries\nTargets (to save): S3\n\nMoving on to the ETL process, we use SQL queries to clean and standardize the raw data. This includes renaming columns, converting string values to numeric types (e.g., float or integer), removing unnecessary commas, and replacing null values with zero. The SQL queries used here are presented as below.\n\n\nShow SQL Query\n\nSELECT sggCd AS sgg_cd\n     , case when sggCd = '11110' then 'AREA_01'\n            when sggCd = '11140' then 'AREA_02'\n            when sggCd = '11170' then 'AREA_03'\n            when sggCd = '11200' then 'AREA_04'\n            when sggCd = '11215' then 'AREA_05'\n            when sggCd = '11230' then 'AREA_06'\n            when sggCd = '11260' then 'AREA_07'\n            when sggCd = '11290' then 'AREA_08'\n            when sggCd = '11305' then 'AREA_09'\n            when sggCd = '11320' then 'AREA_10'\n            when sggCd = '11350' then 'AREA_11'\n            when sggCd = '11380' then 'AREA_12'\n            when sggCd = '11410' then 'AREA_13'\n            when sggCd = '11440' then 'AREA_14'\n            when sggCd = '11470' then 'AREA_15'\n            when sggCd = '11500' then 'AREA_16'\n            when sggCd = '11530' then 'AREA_17'\n            when sggCd = '11545' then 'AREA_18'\n            when sggCd = '11560' then 'AREA_19'\n            when sggCd = '11590' then 'AREA_20'\n            when sggCd = '11620' then 'AREA_21'\n            when sggCd = '11650' then 'AREA_22'\n            when sggCd = '11680' then 'AREA_23'\n            when sggCd = '11710' then 'AREA_24'\n            when sggCd = '11740' then 'AREA_25'\n            else ''\n       end as sgg_nm\n     , umdNm as umd_nm\n     , aptNm as apt_nm\n     , jibun\n     , cast(excluUseAr as float) as exclu_use_ar\n     , cast(dealYear as int) as deal_year\n     , cast(dealMonth as int) as deal_month\n     , cast(dealDay as int) as deal_day\n     , cast(replace(dealAmount, ',', '') as int) as deal_amount\n     , cast(floor as int) as floor\n     , cast(buildYear as int) as build_year\n     , case when cdealType == ' ' then null\n            else cdealType\n       end as cdeal_type\n     , case when cdealDay == ' ' then null\n            else cdealDay\n       end as cdeal_day\n     , case when dealingGbn == ' ' then null\n            else dealingGbn\n       end as dealing_gbn\n     , case when estateAgentSggNm == ' ' then null\n            else estateAgentSggNm\n       end as estate_agent_sgg_nm\n     , case when rgstDate == ' ' then null\n            else rgstDate\n       end as rgst_date\n     , case when aptDong == ' ' then null\n            else aptDong\n       end as apt_dong\n     , case when slerGbn == ' ' then null\n            else slerGbn\n       end as sler_gbn\n     , case when buyerGbn == ' ' then null\n            else buyerGbn\n       end as buyer_gbn\n     , case when landLeaseholdGbn == ' ' then null\n            else landLeaseholdGbn\n       end as land_leasehold_gbn\n     , deal_ymd\n     , lawd_cd\n  FROM apt-trade-raw\nIn AWS Athena, we can make SQL analysis directly on data stored in Amazon S3 without the need to set up a database or server infrastructure. Athena allows us to query structured, semi-structured, or raw data using standard SQL, making it a powerful tool for exploratory data analysis and validation after ETL processes."
  },
  {
    "objectID": "whiteboard01/POST/project_03/index.html",
    "href": "whiteboard01/POST/project_03/index.html",
    "title": "Bayesian Finance",
    "section": "",
    "text": "In generative model we are interested in estimating distribution of \\(x \\in \\mathbb{R}^D\\) by integrating out latent variable \\(z \\in \\mathbb{R}^D\\), that is\n\\[\np(x) = \\int p(x | z)p_z(z)dz\n\\]\nThe integral is usually intractable. In normalizing flow, we pick \\(p_z(z)\\) to be a simple distirbution such as standard normal distribution \\(N(0, I_D)\\), and transform \\(p_z(z)\\) with \\(g: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D\\). We pick \\(g\\) to be diffeomorphism, that is, \\(g\\) is 1. bijective, 2. differentiable, 3. its inverse is differentiable. This allows us to express \\(p(x)\\) using change of variable formula (without any intractable integral involved)\n\\[\np(x) = p_z(g^{-1}(x)) \\left|\\det  \\frac{\\partial g^{-1}(x)}{\\partial x} \\right|\n:=  p_z(f(x)) \\left|\\det  \\frac{\\partial f(x)}{\\partial x} \\right|\n\\] We just defined \\(f := g^{-1}\\). \\(g\\) is referred as generative path because we try to generate \\(x\\) from a simple distribution \\(p_z\\), whereas \\(f\\) is referred as normalising path because we try to degenerate \\(x\\) back to \\(z \\sim p_z\\), where \\(p_z\\) is usually normal distribution.\n\\(f\\) is usually parameterized by neural network to maximise likelihood \\(p(x)\\). \\(f\\) is usually chosen such that the determinant of the Jacobian is easy to calculate. Training and evaluation of density requires normalising path while sampling requires generative path as\n\\[\nx \\sim p(x)\n\\mathrel{\\Leftrightarrow}\nz \\sim p_z(z), x = g(z)\n\\]\nPeople often care about \\(f\\) because fast evaluation of \\(f\\) is needed for fast training. If we need fast sampling, then \\(g\\) is cared.\nIn SBI, coupling flow and autoregressive flow is commonly used."
  },
  {
    "objectID": "whiteboard01/POST/project_03/index.html#coupling-flow",
    "href": "whiteboard01/POST/project_03/index.html#coupling-flow",
    "title": "Bayesian Finance",
    "section": "Coupling flow",
    "text": "Coupling flow\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi &= NN(x^B) \\\\\ny^A &= \\mathbf{h}_\\phi(x^A) = [h_{\\phi_1}(x_1),...,h_{\\phi_d}(x_d)]  \\\\\ny^B &= x^B \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nFirstly, \\(x\\) is split into two dimension such that \\(x^A \\in \\mathbb{R}^d, x^B \\in \\mathbb{R}^{D - d}\\). Then one part is fed into NN: \\(\\mathbb{R}^{D - d} \\rightarrow \\mathbb{R}^k\\) to learn parameter \\(\\phi \\in  \\mathbb{R}^k\\). The NN is referred as conditioner. This \\(\\phi\\) is then coupled with another part \\(x^A\\) through invertible and differentiable function \\(\\mathbf{h}\\) called transformer or coupling function. \\(k\\) depends on the choice of coupling function. Note that transformer is nothing to do with the ‘query-key-value’ transformers. Coupling function is usually applied element-wise. This transformed \\(x^A\\) is then concatenated with \\(x^B\\), which gives output \\(y\\). Its inverse is given by\n\\[\n\\begin{align}\nx^B &= y^B \\\\\n\\phi &= NN(x^B) \\\\\nx^A & = \\mathbf{h}_{\\phi}^{-1}(y^A)\n\\end{align}\n\\]\nThis ‘coupling’ is repeated multiple times, combined with non-stochastic permutation such that all the dimensions \\(i = 1,...,D\\) are interacted to each other. The determinant of Jacobian is given by \\(\\prod_{i=1}^d \\frac{\\partial h_{\\phi_i}(x_i)}{\\partial x_i}\\).\nIn NPE, \\(x \\leftarrow \\theta\\) and the conditioner is also conditioned on data as \\(\\phi = NN(\\theta^B, \\mathcal{D})\\). In NLE, \\(x \\leftarrow \\mathcal{D}\\) and \\(\\phi = NN(\\mathcal{D}^B, \\theta)\\)"
  },
  {
    "objectID": "whiteboard01/POST/project_03/index.html#autoregressive-flow",
    "href": "whiteboard01/POST/project_03/index.html#autoregressive-flow",
    "title": "Bayesian Finance",
    "section": "Autoregressive flow",
    "text": "Autoregressive flow\n\\[\n\\begin{align}\n\\text{For } i &= 2,...,D:\\\\\n\\phi_i &= NN(x_{1:i-1}) \\\\\ny_i &= h_{\\phi_i}(x_i)\\\\\n\\end{align}\n\\]\nwhere \\(\\phi_1\\) is constant. This is very similar to coupling flow except that instead of splitting dimensions into half, dimension is split in autoregressive manners into \\(x_i\\) and \\(x_{1:i-1}\\) mutiple times. In practice, this coupling functions can be implemented without recursion using masking.\nIn NPE, \\(\\phi_1 = NN(\\mathcal{D}), \\phi_i = NN(\\theta_{1:i-1}, \\mathcal{D}), 1 &lt; i \\leq D\\). In NLE, \\(\\phi_1 = NN(\\theta),\\phi_i = NN(\\mathcal{D}_{1:i-1}, \\theta), 1 &lt; i \\leq D\\)"
  },
  {
    "objectID": "whiteboard01/POST/project_03/index.html#coupling-function",
    "href": "whiteboard01/POST/project_03/index.html#coupling-function",
    "title": "Bayesian Finance",
    "section": "Coupling function",
    "text": "Coupling function\n\nAffine\n\\[\nh_{\\phi_{i}}(x_i) = \\alpha_i x_i + \\beta_i, \\quad \\phi_i = (\\alpha_i, \\beta_i)\n\\]\nwhere \\(\\alpha_i &gt; 0, \\beta_i \\in \\mathbb{R}\\). For affine coupling function, \\(k = d \\times 2\\).\n\n\nMonotonic rational quadratic spline\nIn neural spline flow, monotonic rational quadratic spline (RQ-spline) is used for \\(h\\). It allows to model more expressive transformation from \\(x\\) to \\(y\\) and to \\(z\\) at final. Monotonicity is necessary for bijectivity. In a nutshell, \\(h\\) is defined in the interval \\([-B, B]\\) and that interval is split into \\(K\\) bins using \\(K + 1\\) knots, where \\(B\\) and \\(K\\) are hyper-parameters. The height and the width of the bins are trainable parameters \\((\\phi_{h,j}, \\phi_{w, j}), j = 1,...,K\\), and derivatives at each knots are also trainable parameters \\(\\phi_{d,j}, j = 1,...,K-1\\). These parameters defines the RQ-spline. In total, the coupling function have \\(k = (3K - 1) \\times d\\) parameters.\nIn this figure, \\(K = 10\\) is picked."
  },
  {
    "objectID": "whiteboard01/POST/project_03/index.html#bibliography",
    "href": "whiteboard01/POST/project_03/index.html#bibliography",
    "title": "Bayesian Finance",
    "section": "Bibliography",
    "text": "Bibliography\n[1] I. Kobyzev, S. J. D. Prince and M. A. Brubaker, “Normalizing Flows: An Introduction and Review of Current Methods,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 3964-3979.\n[2] Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G. (2019). Neural Spline Flows. Advances in Neural Information Processing Systems, abs/1906.04032. https://proceedings.neurips.cc/paper/2019/hash/7ac71d433f282034e088473244df8c02-Abstract.html\n[3] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & Kothe, U. (2022). BayesFlow: Learning Complex Stochastic Models With Invertible Neural Networks. IEEE Transactions on Neural Networks and Learning Systems, 33(4), 1452–1466."
  },
  {
    "objectID": "whiteboard01/POST/project_03/index.html#appendix",
    "href": "whiteboard01/POST/project_03/index.html#appendix",
    "title": "Bayesian Finance",
    "section": "Appendix",
    "text": "Appendix\n\nDual coupling\nSometimes, one block of coupling is defined as two coupling as in [3] such that both split is transformed by coupling function in one unit of coupling.\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi_1 &= NN_1(x^B) \\\\\ny^A &= \\mathbf{h}_{\\phi_1}(x^A) \\\\\n\\phi_2 &=  NN_2(y^A)\\\\\ny^B &= \\mathbf{h}_{\\phi_2}(x^B) \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nThis is same as applying single coupling flow followed by permutation, which swap \\(x^A\\) and \\(x^B\\) and applying another coupling flow.\n\n\nWhen \\(D = 1\\)\nIn generative model \\(x\\) is usually something high dimensional like image. But in SBI, \\(D\\) could be 1. Coupling flow could be applied in such case. For example in NPE, if \\(\\theta \\in \\mathbb{R}\\) and consider only one coupling, then we have\n\\[\n\\begin{align}\n\\phi &= NN(\\mathcal{D}) \\\\\nz &= \\mathbf{h}_{\\phi}(\\theta)\n\\end{align}\n\\]\nIn this case, \\(\\theta\\) is not going to be coupled with another split of \\(\\theta\\) because \\(\\theta\\) is just one-dimensional, but it is still coupled with \\(\\mathcal{D}\\).\nIn this case, naturally, coupling flow is same as autoregressive flow given same conditioner and coupling function."
  },
  {
    "objectID": "whiteboard01/POST/project_01/index.html",
    "href": "whiteboard01/POST/project_01/index.html",
    "title": "Gaussian Process",
    "section": "",
    "text": "In generative model we are interested in estimating distribution of \\(x \\in \\mathbb{R}^D\\) by integrating out latent variable \\(z \\in \\mathbb{R}^D\\), that is\n\\[\np(x) = \\int p(x | z)p_z(z)dz\n\\]\nThe integral is usually intractable. In normalizing flow, we pick \\(p_z(z)\\) to be a simple distirbution such as standard normal distribution \\(N(0, I_D)\\), and transform \\(p_z(z)\\) with \\(g: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D\\). We pick \\(g\\) to be diffeomorphism, that is, \\(g\\) is 1. bijective, 2. differentiable, 3. its inverse is differentiable. This allows us to express \\(p(x)\\) using change of variable formula (without any intractable integral involved)\n\\[\np(x) = p_z(g^{-1}(x)) \\left|\\det  \\frac{\\partial g^{-1}(x)}{\\partial x} \\right|\n:=  p_z(f(x)) \\left|\\det  \\frac{\\partial f(x)}{\\partial x} \\right|\n\\] We just defined \\(f := g^{-1}\\). \\(g\\) is referred as generative path because we try to generate \\(x\\) from a simple distribution \\(p_z\\), whereas \\(f\\) is referred as normalising path because we try to degenerate \\(x\\) back to \\(z \\sim p_z\\), where \\(p_z\\) is usually normal distribution.\n\\(f\\) is usually parameterized by neural network to maximise likelihood \\(p(x)\\). \\(f\\) is usually chosen such that the determinant of the Jacobian is easy to calculate. Training and evaluation of density requires normalising path while sampling requires generative path as\n\\[\nx \\sim p(x)\n\\mathrel{\\Leftrightarrow}\nz \\sim p_z(z), x = g(z)\n\\]\nPeople often care about \\(f\\) because fast evaluation of \\(f\\) is needed for fast training. If we need fast sampling, then \\(g\\) is cared.\nIn SBI, coupling flow and autoregressive flow is commonly used."
  },
  {
    "objectID": "whiteboard01/POST/project_01/index.html#coupling-flow",
    "href": "whiteboard01/POST/project_01/index.html#coupling-flow",
    "title": "Gaussian Process",
    "section": "Coupling flow",
    "text": "Coupling flow\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi &= NN(x^B) \\\\\ny^A &= \\mathbf{h}_\\phi(x^A) = [h_{\\phi_1}(x_1),...,h_{\\phi_d}(x_d)]  \\\\\ny^B &= x^B \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nFirstly, \\(x\\) is split into two dimension such that \\(x^A \\in \\mathbb{R}^d, x^B \\in \\mathbb{R}^{D - d}\\). Then one part is fed into NN: \\(\\mathbb{R}^{D - d} \\rightarrow \\mathbb{R}^k\\) to learn parameter \\(\\phi \\in  \\mathbb{R}^k\\). The NN is referred as conditioner. This \\(\\phi\\) is then coupled with another part \\(x^A\\) through invertible and differentiable function \\(\\mathbf{h}\\) called transformer or coupling function. \\(k\\) depends on the choice of coupling function. Note that transformer is nothing to do with the ‘query-key-value’ transformers. Coupling function is usually applied element-wise. This transformed \\(x^A\\) is then concatenated with \\(x^B\\), which gives output \\(y\\). Its inverse is given by\n\\[\n\\begin{align}\nx^B &= y^B \\\\\n\\phi &= NN(x^B) \\\\\nx^A & = \\mathbf{h}_{\\phi}^{-1}(y^A)\n\\end{align}\n\\]\nThis ‘coupling’ is repeated multiple times, combined with non-stochastic permutation such that all the dimensions \\(i = 1,...,D\\) are interacted to each other. The determinant of Jacobian is given by \\(\\prod_{i=1}^d \\frac{\\partial h_{\\phi_i}(x_i)}{\\partial x_i}\\).\nIn NPE, \\(x \\leftarrow \\theta\\) and the conditioner is also conditioned on data as \\(\\phi = NN(\\theta^B, \\mathcal{D})\\). In NLE, \\(x \\leftarrow \\mathcal{D}\\) and \\(\\phi = NN(\\mathcal{D}^B, \\theta)\\)"
  },
  {
    "objectID": "whiteboard01/POST/project_01/index.html#autoregressive-flow",
    "href": "whiteboard01/POST/project_01/index.html#autoregressive-flow",
    "title": "Gaussian Process",
    "section": "Autoregressive flow",
    "text": "Autoregressive flow\n\\[\n\\begin{align}\n\\text{For } i &= 2,...,D:\\\\\n\\phi_i &= NN(x_{1:i-1}) \\\\\ny_i &= h_{\\phi_i}(x_i)\\\\\n\\end{align}\n\\]\nwhere \\(\\phi_1\\) is constant. This is very similar to coupling flow except that instead of splitting dimensions into half, dimension is split in autoregressive manners into \\(x_i\\) and \\(x_{1:i-1}\\) mutiple times. In practice, this coupling functions can be implemented without recursion using masking.\nIn NPE, \\(\\phi_1 = NN(\\mathcal{D}), \\phi_i = NN(\\theta_{1:i-1}, \\mathcal{D}), 1 &lt; i \\leq D\\). In NLE, \\(\\phi_1 = NN(\\theta),\\phi_i = NN(\\mathcal{D}_{1:i-1}, \\theta), 1 &lt; i \\leq D\\)"
  },
  {
    "objectID": "whiteboard01/POST/project_01/index.html#coupling-function",
    "href": "whiteboard01/POST/project_01/index.html#coupling-function",
    "title": "Gaussian Process",
    "section": "Coupling function",
    "text": "Coupling function\n\nAffine\n\\[\nh_{\\phi_{i}}(x_i) = \\alpha_i x_i + \\beta_i, \\quad \\phi_i = (\\alpha_i, \\beta_i)\n\\]\nwhere \\(\\alpha_i &gt; 0, \\beta_i \\in \\mathbb{R}\\). For affine coupling function, \\(k = d \\times 2\\).\n\n\nMonotonic rational quadratic spline\nIn neural spline flow, monotonic rational quadratic spline (RQ-spline) is used for \\(h\\). It allows to model more expressive transformation from \\(x\\) to \\(y\\) and to \\(z\\) at final. Monotonicity is necessary for bijectivity. In a nutshell, \\(h\\) is defined in the interval \\([-B, B]\\) and that interval is split into \\(K\\) bins using \\(K + 1\\) knots, where \\(B\\) and \\(K\\) are hyper-parameters. The height and the width of the bins are trainable parameters \\((\\phi_{h,j}, \\phi_{w, j}), j = 1,...,K\\), and derivatives at each knots are also trainable parameters \\(\\phi_{d,j}, j = 1,...,K-1\\). These parameters defines the RQ-spline. In total, the coupling function have \\(k = (3K - 1) \\times d\\) parameters.\nIn this figure, \\(K = 10\\) is picked."
  },
  {
    "objectID": "whiteboard01/POST/project_01/index.html#bibliography",
    "href": "whiteboard01/POST/project_01/index.html#bibliography",
    "title": "Gaussian Process",
    "section": "Bibliography",
    "text": "Bibliography\n[1] I. Kobyzev, S. J. D. Prince and M. A. Brubaker, “Normalizing Flows: An Introduction and Review of Current Methods,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 3964-3979.\n[2] Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G. (2019). Neural Spline Flows. Advances in Neural Information Processing Systems, abs/1906.04032. https://proceedings.neurips.cc/paper/2019/hash/7ac71d433f282034e088473244df8c02-Abstract.html\n[3] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & Kothe, U. (2022). BayesFlow: Learning Complex Stochastic Models With Invertible Neural Networks. IEEE Transactions on Neural Networks and Learning Systems, 33(4), 1452–1466."
  },
  {
    "objectID": "whiteboard01/POST/project_01/index.html#appendix",
    "href": "whiteboard01/POST/project_01/index.html#appendix",
    "title": "Gaussian Process",
    "section": "Appendix",
    "text": "Appendix\n\nDual coupling\nSometimes, one block of coupling is defined as two coupling as in [3] such that both split is transformed by coupling function in one unit of coupling.\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi_1 &= NN_1(x^B) \\\\\ny^A &= \\mathbf{h}_{\\phi_1}(x^A) \\\\\n\\phi_2 &=  NN_2(y^A)\\\\\ny^B &= \\mathbf{h}_{\\phi_2}(x^B) \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nThis is same as applying single coupling flow followed by permutation, which swap \\(x^A\\) and \\(x^B\\) and applying another coupling flow.\n\n\nWhen \\(D = 1\\)\nIn generative model \\(x\\) is usually something high dimensional like image. But in SBI, \\(D\\) could be 1. Coupling flow could be applied in such case. For example in NPE, if \\(\\theta \\in \\mathbb{R}\\) and consider only one coupling, then we have\n\\[\n\\begin{align}\n\\phi &= NN(\\mathcal{D}) \\\\\nz &= \\mathbf{h}_{\\phi}(\\theta)\n\\end{align}\n\\]\nIn this case, \\(\\theta\\) is not going to be coupled with another split of \\(\\theta\\) because \\(\\theta\\) is just one-dimensional, but it is still coupled with \\(\\mathcal{D}\\).\nIn this case, naturally, coupling flow is same as autoregressive flow given same conditioner and coupling function."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "My Research Work",
    "section": "",
    "text": "PhD Thesis: Risk Premium Development with Model Risk\n\nDCU DORAS | Code | \nMotivated by the case of catastrophic financial losses in the insurance industry, this thesis investigates Bayesian approaches to address diverse model risks in risk premium prediction. Unlike classical actuarial methods that rely solely on data, Bayesian models incorporate parameter knowledge, offering flexibility in handling erroneous data issue. We leverage this advantage to link Bayesian parametric/nonparametric frameworks with state-of-art strategies for managing incomplete data issues, such as Missingness at Random (MAR) and Non-Differential Berkson (NDB) mismeasurement. Additionally, we address other key analytical challenges, including heterogeneity, convolution, and scalability.\n\n\n\nBayesian Nonparametric Log Skew-Normal Mixture with Missing At Random Covariates in Insurance Practice\n\nEconometrics | Code\nWe propose novel connections among a covariate-dependent Dirichlet process mixture, log-normal convolution, and missing covariate imputation. As a generative approach, our framework models the joint of outcome and covariates, which allows us to impute missing covariates under the assumption of missingness at random.\n\n\n\nParametric Hierarchical Bayes with Non-Differential Berkson Covariates in Insurance Practice\n\nApplied Sciences | Code\nWe explore connections between Bayesian hierarchical modeling, partial pooling techniques, and the Gustafson correction method for mismeasured covariates. Focusing on Non-Differential Berkson (NDB) mismeasurement, we propose an approach that corrects such errors without relying on gold standard data. We discover the unique prior knowledge regarding the variance of the NDB errors, and utilize it to adjust the biased parameter estimates built upon the NDB covariate.\n\n\n\nUsing Tableau and Google Map API for Understanding the Impact of Walkability on Dublin City\n\narXiv Preprint | Code\nWe utilize the Google Map API and Tableau to visualize the less walkable areas across Dublin city and using WLS regression, we assess the effects of unwalkability on house prices in Dublin, thus quantifying the importance of walkable areas from an economic perspective."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Minkun Kim",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \n\nWelcome to my website!\nI am a scholar dedicated to understanding the complexities of our stochastic world. This website offers a window into my research and selected findings accumulated over years of my academic and applied work. My research interests lie at the intersection of Bayesian computing, quantitative finance, data engineering, and energy markets. I also have a personal interest in data-driven urban transformation. Here, you’ll find a selection of my work, along with recent insights and ongoing developments from my current studies.\nPlease feel free to contact me if you have any questions or would like to discuss potential projects.\n\n\n\nInterests\nValuation/Risk Modeling  Stochastic Modeling  Nonparametric Bayesian Computing  Data Engineering  Data-Driven Design \n\n\nEducation\n\nPh.D Computing  Dublin City University, Ireland \nM.Sc Financial Mathematics  University of York, UK \nM.Sc Sustainable Energy System  Ruhr University Bochum, Germany \nM.Sc Data Analytics  Dublin City University, Ireland \nB.Sc Urban Engineering  Hongik University, South Korea"
  },
  {
    "objectID": "whiteboard01/index.html",
    "href": "whiteboard01/index.html",
    "title": "My Financial Math",
    "section": "",
    "text": "The mathematical Finance is….\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process\n\n\n\nPredictive Bayesian\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\n\n\n\n\n\n\n\nDirichlet Process\n\n\n\nPredictive Bayesian\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Finance\n\n\n\nBayesian approach\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "whiteboard01/POST/project_02/index.html",
    "href": "whiteboard01/POST/project_02/index.html",
    "title": "Dirichlet Process",
    "section": "",
    "text": "In generative model we are interested in estimating distribution of \\(x \\in \\mathbb{R}^D\\) by integrating out latent variable \\(z \\in \\mathbb{R}^D\\), that is\n\\[\np(x) = \\int p(x | z)p_z(z)dz\n\\]\nThe integral is usually intractable. In normalizing flow, we pick \\(p_z(z)\\) to be a simple distirbution such as standard normal distribution \\(N(0, I_D)\\), and transform \\(p_z(z)\\) with \\(g: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D\\). We pick \\(g\\) to be diffeomorphism, that is, \\(g\\) is 1. bijective, 2. differentiable, 3. its inverse is differentiable. This allows us to express \\(p(x)\\) using change of variable formula (without any intractable integral involved)\n\\[\np(x) = p_z(g^{-1}(x)) \\left|\\det  \\frac{\\partial g^{-1}(x)}{\\partial x} \\right|\n:=  p_z(f(x)) \\left|\\det  \\frac{\\partial f(x)}{\\partial x} \\right|\n\\] We just defined \\(f := g^{-1}\\). \\(g\\) is referred as generative path because we try to generate \\(x\\) from a simple distribution \\(p_z\\), whereas \\(f\\) is referred as normalising path because we try to degenerate \\(x\\) back to \\(z \\sim p_z\\), where \\(p_z\\) is usually normal distribution.\n\\(f\\) is usually parameterized by neural network to maximise likelihood \\(p(x)\\). \\(f\\) is usually chosen such that the determinant of the Jacobian is easy to calculate. Training and evaluation of density requires normalising path while sampling requires generative path as\n\\[\nx \\sim p(x)\n\\mathrel{\\Leftrightarrow}\nz \\sim p_z(z), x = g(z)\n\\]\nPeople often care about \\(f\\) because fast evaluation of \\(f\\) is needed for fast training. If we need fast sampling, then \\(g\\) is cared.\nIn SBI, coupling flow and autoregressive flow is commonly used."
  },
  {
    "objectID": "whiteboard01/POST/project_02/index.html#coupling-flow",
    "href": "whiteboard01/POST/project_02/index.html#coupling-flow",
    "title": "Dirichlet Process",
    "section": "Coupling flow",
    "text": "Coupling flow\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi &= NN(x^B) \\\\\ny^A &= \\mathbf{h}_\\phi(x^A) = [h_{\\phi_1}(x_1),...,h_{\\phi_d}(x_d)]  \\\\\ny^B &= x^B \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nFirstly, \\(x\\) is split into two dimension such that \\(x^A \\in \\mathbb{R}^d, x^B \\in \\mathbb{R}^{D - d}\\). Then one part is fed into NN: \\(\\mathbb{R}^{D - d} \\rightarrow \\mathbb{R}^k\\) to learn parameter \\(\\phi \\in  \\mathbb{R}^k\\). The NN is referred as conditioner. This \\(\\phi\\) is then coupled with another part \\(x^A\\) through invertible and differentiable function \\(\\mathbf{h}\\) called transformer or coupling function. \\(k\\) depends on the choice of coupling function. Note that transformer is nothing to do with the ‘query-key-value’ transformers. Coupling function is usually applied element-wise. This transformed \\(x^A\\) is then concatenated with \\(x^B\\), which gives output \\(y\\). Its inverse is given by\n\\[\n\\begin{align}\nx^B &= y^B \\\\\n\\phi &= NN(x^B) \\\\\nx^A & = \\mathbf{h}_{\\phi}^{-1}(y^A)\n\\end{align}\n\\]\nThis ‘coupling’ is repeated multiple times, combined with non-stochastic permutation such that all the dimensions \\(i = 1,...,D\\) are interacted to each other. The determinant of Jacobian is given by \\(\\prod_{i=1}^d \\frac{\\partial h_{\\phi_i}(x_i)}{\\partial x_i}\\).\nIn NPE, \\(x \\leftarrow \\theta\\) and the conditioner is also conditioned on data as \\(\\phi = NN(\\theta^B, \\mathcal{D})\\). In NLE, \\(x \\leftarrow \\mathcal{D}\\) and \\(\\phi = NN(\\mathcal{D}^B, \\theta)\\)"
  },
  {
    "objectID": "whiteboard01/POST/project_02/index.html#autoregressive-flow",
    "href": "whiteboard01/POST/project_02/index.html#autoregressive-flow",
    "title": "Dirichlet Process",
    "section": "Autoregressive flow",
    "text": "Autoregressive flow\n\\[\n\\begin{align}\n\\text{For } i &= 2,...,D:\\\\\n\\phi_i &= NN(x_{1:i-1}) \\\\\ny_i &= h_{\\phi_i}(x_i)\\\\\n\\end{align}\n\\]\nwhere \\(\\phi_1\\) is constant. This is very similar to coupling flow except that instead of splitting dimensions into half, dimension is split in autoregressive manners into \\(x_i\\) and \\(x_{1:i-1}\\) mutiple times. In practice, this coupling functions can be implemented without recursion using masking.\nIn NPE, \\(\\phi_1 = NN(\\mathcal{D}), \\phi_i = NN(\\theta_{1:i-1}, \\mathcal{D}), 1 &lt; i \\leq D\\). In NLE, \\(\\phi_1 = NN(\\theta),\\phi_i = NN(\\mathcal{D}_{1:i-1}, \\theta), 1 &lt; i \\leq D\\)"
  },
  {
    "objectID": "whiteboard01/POST/project_02/index.html#coupling-function",
    "href": "whiteboard01/POST/project_02/index.html#coupling-function",
    "title": "Dirichlet Process",
    "section": "Coupling function",
    "text": "Coupling function\n\nAffine\n\\[\nh_{\\phi_{i}}(x_i) = \\alpha_i x_i + \\beta_i, \\quad \\phi_i = (\\alpha_i, \\beta_i)\n\\]\nwhere \\(\\alpha_i &gt; 0, \\beta_i \\in \\mathbb{R}\\). For affine coupling function, \\(k = d \\times 2\\).\n\n\nMonotonic rational quadratic spline\nIn neural spline flow, monotonic rational quadratic spline (RQ-spline) is used for \\(h\\). It allows to model more expressive transformation from \\(x\\) to \\(y\\) and to \\(z\\) at final. Monotonicity is necessary for bijectivity. In a nutshell, \\(h\\) is defined in the interval \\([-B, B]\\) and that interval is split into \\(K\\) bins using \\(K + 1\\) knots, where \\(B\\) and \\(K\\) are hyper-parameters. The height and the width of the bins are trainable parameters \\((\\phi_{h,j}, \\phi_{w, j}), j = 1,...,K\\), and derivatives at each knots are also trainable parameters \\(\\phi_{d,j}, j = 1,...,K-1\\). These parameters defines the RQ-spline. In total, the coupling function have \\(k = (3K - 1) \\times d\\) parameters.\nIn this figure, \\(K = 10\\) is picked."
  },
  {
    "objectID": "whiteboard01/POST/project_02/index.html#bibliography",
    "href": "whiteboard01/POST/project_02/index.html#bibliography",
    "title": "Dirichlet Process",
    "section": "Bibliography",
    "text": "Bibliography\n[1] I. Kobyzev, S. J. D. Prince and M. A. Brubaker, “Normalizing Flows: An Introduction and Review of Current Methods,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 3964-3979.\n[2] Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G. (2019). Neural Spline Flows. Advances in Neural Information Processing Systems, abs/1906.04032. https://proceedings.neurips.cc/paper/2019/hash/7ac71d433f282034e088473244df8c02-Abstract.html\n[3] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & Kothe, U. (2022). BayesFlow: Learning Complex Stochastic Models With Invertible Neural Networks. IEEE Transactions on Neural Networks and Learning Systems, 33(4), 1452–1466."
  },
  {
    "objectID": "whiteboard01/POST/project_02/index.html#appendix",
    "href": "whiteboard01/POST/project_02/index.html#appendix",
    "title": "Dirichlet Process",
    "section": "Appendix",
    "text": "Appendix\n\nDual coupling\nSometimes, one block of coupling is defined as two coupling as in [3] such that both split is transformed by coupling function in one unit of coupling.\n\\[\n\\begin{align}\nx &= [x^A, x^B] \\\\\n\\phi_1 &= NN_1(x^B) \\\\\ny^A &= \\mathbf{h}_{\\phi_1}(x^A) \\\\\n\\phi_2 &=  NN_2(y^A)\\\\\ny^B &= \\mathbf{h}_{\\phi_2}(x^B) \\\\\ny &= [y^A, y^B]\n\\end{align}\n\\]\nThis is same as applying single coupling flow followed by permutation, which swap \\(x^A\\) and \\(x^B\\) and applying another coupling flow.\n\n\nWhen \\(D = 1\\)\nIn generative model \\(x\\) is usually something high dimensional like image. But in SBI, \\(D\\) could be 1. Coupling flow could be applied in such case. For example in NPE, if \\(\\theta \\in \\mathbb{R}\\) and consider only one coupling, then we have\n\\[\n\\begin{align}\n\\phi &= NN(\\mathcal{D}) \\\\\nz &= \\mathbf{h}_{\\phi}(\\theta)\n\\end{align}\n\\]\nIn this case, \\(\\theta\\) is not going to be coupled with another split of \\(\\theta\\) because \\(\\theta\\) is just one-dimensional, but it is still coupled with \\(\\mathcal{D}\\).\nIn this case, naturally, coupling flow is same as autoregressive flow given same conditioner and coupling function."
  },
  {
    "objectID": "whiteboard02/index.html",
    "href": "whiteboard02/index.html",
    "title": "My Data Engineering",
    "section": "",
    "text": "In energy finance, being a quant isn’t just about building models — it’s about making sure those models are fed with clean, timely, and scalable data. Our models are only as good as the data that drives them. Model accuracy depends on data quality, not just mathematical skill! We are dealing with massive amounts of real-time and historical data — electricity load curves, weather forecasts, fuel prices, grid constraints, power plant outages, carbon credit prices, etc. Without a well-designed ETL process or pipeline, this data is messy, delayed, or incomplete, making any model unreliable. That’s where data engineering skills come in.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Pipelining with AWS\n\n\n\ndata storage\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\n\n\n\n\n\n\n\nData Processing with Spark\n\n\n\ndata processing\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Monitoring for Casting RUL Analysis\n\n\n\ndata quality monitoring\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "whiteboard02/POST/project_2/index.html",
    "href": "whiteboard02/POST/project_2/index.html",
    "title": "Data Processing with Spark",
    "section": "",
    "text": "“Imagine you have a huge pile of Lego pieces and you want to build lots of different things. Hadoop writes down every single step in a notebook (disk I/O) before it adds even one Lego piece. It builds a little bit, writes it down, builds a little more, writes again. This makes sure it doesn’t forget anything, but it also means it is very slow because it keeps stopping to write.\nSpark works with a super strong memory. It can hold lots of steps in its head without writing them down all the time, so it builds things much faster. Spark also supports richer workloads (counting, searching, learning, etc.) in a single framework, including SQL, streaming, machine learning, etc. This means it can switch between tasks easily — one moment it’s sorting Lego colors, next it’s counting pieces, next it’s building a robot — all without slowing down. Hadoop needs a whole new notebook step each time it changes what it’s doing.\n\n\n\nSpark Architecture & two main actors\n\n\nIn Spark, the Driver Program is the central coordinator of a job. It constructs the execution plan, schedules tasks, and maintains the metadata and state of the application. The Executors are the distributed workers that actually run those tasks, store data in memory, and return results back to the Driver. Overseeing the whole system is the Cluster Manager—such as YARN, Kubernetes, or the standalone manager—which allocates resources to the application by deciding how many executors to launch and on which nodes. Together, the Driver provides the control logic, the Executors provide the computational power, and the Cluster Manager provides the resources needed for distributed execution."
  },
  {
    "objectID": "whiteboard02/POST/project_2/index.html#spark-vs-hadoop",
    "href": "whiteboard02/POST/project_2/index.html#spark-vs-hadoop",
    "title": "Data Processing with Spark",
    "section": "",
    "text": "“Imagine you have a huge pile of Lego pieces and you want to build lots of different things. Hadoop writes down every single step in a notebook (disk I/O) before it adds even one Lego piece. It builds a little bit, writes it down, builds a little more, writes again. This makes sure it doesn’t forget anything, but it also means it is very slow because it keeps stopping to write.\nSpark works with a super strong memory. It can hold lots of steps in its head without writing them down all the time, so it builds things much faster. Spark also supports richer workloads (counting, searching, learning, etc.) in a single framework, including SQL, streaming, machine learning, etc. This means it can switch between tasks easily — one moment it’s sorting Lego colors, next it’s counting pieces, next it’s building a robot — all without slowing down. Hadoop needs a whole new notebook step each time it changes what it’s doing.\n\n\n\nSpark Architecture & two main actors\n\n\nIn Spark, the Driver Program is the central coordinator of a job. It constructs the execution plan, schedules tasks, and maintains the metadata and state of the application. The Executors are the distributed workers that actually run those tasks, store data in memory, and return results back to the Driver. Overseeing the whole system is the Cluster Manager—such as YARN, Kubernetes, or the standalone manager—which allocates resources to the application by deciding how many executors to launch and on which nodes. Together, the Driver provides the control logic, the Executors provide the computational power, and the Cluster Manager provides the resources needed for distributed execution."
  },
  {
    "objectID": "whiteboard03/index.html",
    "href": "whiteboard03/index.html",
    "title": "My Sustainability",
    "section": "",
    "text": "In Energy Economics, sustainability is….\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEU-Sustainability Reporting - E1\n\n\n\nESRS\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\n\n\n\n\n\n\n\nEU-Sustainability Reporting - S1\n\n\n\nESRS\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\n\n\n\n\n\n\n\nDouble Materiality Assessment\n\n\n\nESRS\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\n\n\n\n\n\n\n\nEU-Taxonomy Analysis\n\n\n\nESRS\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nMinkun Kim\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "whiteboard03/POST/project_2/index.html",
    "href": "whiteboard03/POST/project_2/index.html",
    "title": "EU-Sustainability Reporting - S1",
    "section": "",
    "text": "Be transparent: How much GREEN are you?\nESRS (EU Sustainability Reporting Standards) provides mandatory reporting standards under the CSRD (Corporate Sustainability Reporting Directive) in the EU. ESRS define how companies must report on sustainability-related impacts, risks, and opportunities. They include two cross-cutting standards (ESRS 1 and ESRS 2) that explain the general principles, reporting structure, and required company-wide disclosures. The environmental standards (E1–E5) cover climate change, pollution, water and marine resources, biodiversity, and resource use and circular economy. The social standards (S1–S4) focus on a company’s own workforce, workers in the value chain, affected communities, and consumers. Finally, the governance standards (G1) addresses ethical business conduct, including corruption, lobbying, and transparency."
  },
  {
    "objectID": "whiteboard03/POST/project_2/index.html#a.-s1-disclosure-prep-with-excel",
    "href": "whiteboard03/POST/project_2/index.html#a.-s1-disclosure-prep-with-excel",
    "title": "EU-Sustainability Reporting - S1",
    "section": "[A]. S1 Disclosure Prep with Excel",
    "text": "[A]. S1 Disclosure Prep with Excel\nLet’s prepare the relevant tables or graphs in Excel for the KPIs and breakdowns (by gender, country, contract type) specified in the Application Requirements (AR) shown in Figure 2. The excel spreadsheet contains 14 columns and 30,798 rows describing employee information for ClimAID Services AG in Bochum.\n\n\n\n\n\n\n\nNote\n\n\n\n\nchoose: Data &gt; Filter (by Gender) &gt; Ctrl+Shift+Down for basic tasks.\nRight next to the last column, create a dummy column (with 1) called “headcount” and activate Pivot table\n\nAdd a new sheet, and from it, choose: Insert &gt; PivotTable: ctrl+a \nA pivotTable panel appears. In the panel on the right, choose the variables to create a table. \nby Gender ⇒\nby Country ⇒\nby Contract type ⇒\n\n\n[S1-5] Characteristics of the undertaking’s employees ⇒\n\n[S1-7] Collective bargaining coverage and social dialogue ⇒\n\n[S1-8] Diversity metrics ⇒\n\n[S1-9] Adequate wages ⇒\n\n[S1-15] Remuneration metrics ⇒"
  },
  {
    "objectID": "whiteboard03/POST/project_2/index.html#b.-difficulties-with-s1",
    "href": "whiteboard03/POST/project_2/index.html#b.-difficulties-with-s1",
    "title": "EU-Sustainability Reporting - S1",
    "section": "[B]. Difficulties with S1",
    "text": "[B]. Difficulties with S1\nLet me name difficulties we could likely encounter when working with files like these to prepare ESRS disclosures (for social matters specifically and ESG matters in general). Let me describe briefly how could we mitigate these problems.\n\nGeneral file issues for ESG reporting\n\nData may not be available in one file so needs to be collected from different departments/countries. ⇒ Plan sufficient collecting time.\nMultiple files, not structured or formatted in the same way ⇒ To merge files, provide with data template to fill in.\nTypos, missing lines, inconsistencies in data ⇒ employ sanity check, checking by filltering, etc.\n\nPotential file issues for S1\n\nConfidentiality issues due to sensitivity of data on payment and personal information ⇒ anonymize data or let local data managers carry out calculations; firm-wide data would then be aggregated\nCalculating AVG or fluctuations might be difficult, if file is based on end-of-year data ⇒ obtain data that covers intra-year flow of personell\nData might need to be broken down into more detail (e.g., salary broken down into fixed and variable components) ⇒ plan necessary analyses ahead to avoid receiving data in an aggregated format\nFile may not cover individual deals (e.g. sabatticals, maternal/paternal leave, sick days, …) ⇒ plan necessary analyses ahead to avoid receiving data in an aggregated format"
  },
  {
    "objectID": "whiteboard03/POST/project_2/index.html#bibliography",
    "href": "whiteboard03/POST/project_2/index.html#bibliography",
    "title": "EU-Sustainability Reporting - S1",
    "section": "Bibliography",
    "text": "Bibliography\n[1] EFRAG: Draft ESRS S1 – Own Workforce, https://www.efrag.org/sites/default/files/media/document/2025-12/November_2025_ESRS_S1.pdf (November 2025)."
  },
  {
    "objectID": "whiteboard03/POST/project_4/index.html",
    "href": "whiteboard03/POST/project_4/index.html",
    "title": "EU-Taxonomy Analysis",
    "section": "",
    "text": "ONLY ``Enviropnmental objectives”..EU-Taxonomy does not address S or G objectives. what counts as an environmentally sustainable economic activity? WHat is your % of business (economic activities) that is really GREEN?\nEU-Taxonomy is developed to create a common language on what Green activity is…to reduce ``green washing” and improve comparability. Using EU-Taxonomy, a firm can determine what economic activities are ENVORONMENTALLY (not Social or Governance-wise) sustainable under the conditions: - Technical Screening Criteria (to see significance) - DNSH (not to significantly harm others) - Meeting Minimum Social Safeguards (to respect human right)\nTo meet the E-objectives - Climate, Water, Pollution, Biodiversity, Circular Econ -, a firm discloses what % of their business that is Green…in terms of Revenue (what a firm does now), CAPEX (what a firm does for future), OPEX (how a firm supports its operation)."
  },
  {
    "objectID": "whiteboard03/POST/project_4/index.html#a.-if-being-mentioned-taxonomy-eligibility",
    "href": "whiteboard03/POST/project_4/index.html#a.-if-being-mentioned-taxonomy-eligibility",
    "title": "EU-Taxonomy Analysis",
    "section": "[A]. If being mentioned: taxonomy-eligibility",
    "text": "[A]. If being mentioned: taxonomy-eligibility\nAccording to the Disclosures Delegated Act (EU) 2021, a taxonomy-eligible economic activity refers to an economic activity that is described in the Delegated Acts where technical screening criteria are defined for each environmental goal (Art.1 par.5 Disclosures Delegated Act). All remaining economic activities, that are not described in the Delegated Act, are taxonomy non-eligible (Art.1 par.6 Disclosures Delegated Act).\nFor Gensler AG, we are preparing their EU Taxonomy disclosure with regard to taxonomy-eligibility. The activities are listed in Table 1. We consider if these economic activities are described in the Climate Delegated Act (Comm. Del. Regulation (EU) 2021/2139).\n\n\n\nTable 1. Gensler AG segments and activities\n\n\n\n\n\n\n\n\nNote\n\n\n\nactivity 1 Construction of new building ⇒ {taxonomy-eligible}: described as activity 7.1 of the Climate Delegated Act\nactivity 2 Renovation of existing building ⇒ {taxonomy-eligible}: described as activity 7.2 of the Climate Delegated Act\nactivity 3 House keeping contracts ⇒ {taxonomy-non-eligible}: not described in any delegated act (yet)\nactivity 4 Retail sales of construction materials ⇒ {taxonomy-non-eligible}: not described in any delegated act (yet)\nactivity 5 General management ⇒ {taxonomy-non-eligible}: not described in any delegated act (yet)"
  },
  {
    "objectID": "whiteboard03/POST/project_4/index.html#b.-if-meeting-criteria-taxonomy-alignment",
    "href": "whiteboard03/POST/project_4/index.html#b.-if-meeting-criteria-taxonomy-alignment",
    "title": "EU-Taxonomy Analysis",
    "section": "[B]. If meeting criteria: taxonomy-alignment",
    "text": "[B]. If meeting criteria: taxonomy-alignment\nAccording to the Disclosures Delegated Act (EU) 2021/2178, a taxonomy-aligned economic activity refers to an economic activity that complies with the requirements laid down in Art.3 of Commission Delegated Regulation (EU) 2020/852 for each environmental goal (Art.1 par.2 Disclosures Delegated Act).\n\nArt.3 of Regulation (EU) 2020/852: An economic activity shall qualify as environmentally sustainable where that your economic activity….\n\n\ncontributes substantially to one or more of the six environmental objectives.\n\n\ndoes not significantly harm any of the environmental objectives (DNSH).\n\n\nis carried out in compliance with the minimum social safeguards (basic human rights, labor, and anti-corruption standards based on OECD, UN, and ILO principles).\n\nComplies with the technical screening criteria set out in Delegated Acts for the respective environmental objective.\n\n\nGensler AG is preparing their EU Taxonomy disclosure with regard to taxonomy-alignment. In Table 2, activities are listed. Considering the economic activities described in the Climate Delegated Act (Comm. Del. Regulation (EU) 2021/2139), let’s see if they are taxonomy-aligned, and meet the climate-change mitigation goal\n\n\n\nTable 2. Gensler AG activity description\n\n\n\n\n\n\n\n\nNote\n\n\n\nactivity 1 Construction of new building ⇒ {taxonomy-aligned}:\nactivity 2 Renovation of existing building ⇒ {taxonomy-not-aligned}:\nactivity 3 House keeping contracts ⇒ {NA}:\nactivity 4 Retail sales of construction materials ⇒ {NA}:\nactivity 5 General management ⇒ {NA}:"
  },
  {
    "objectID": "whiteboard03/POST/project_4/index.html#c.-calculating-the-taxonomy-eligiblealigned-quotas",
    "href": "whiteboard03/POST/project_4/index.html#c.-calculating-the-taxonomy-eligiblealigned-quotas",
    "title": "EU-Taxonomy Analysis",
    "section": "[C]. Calculating the taxonomy-eligible/aligned quotas",
    "text": "[C]. Calculating the taxonomy-eligible/aligned quotas\nTo comply with the EU Taxonomy, Gensler AG must report the taxonomy quotas for the current reporting year. Officially, the reporting output shall be presented as suggested by the regulation (Table 3). However, Gensler AG is first preparing a rough draft for filling in this table. Let’s assume that activity 1. (Construction of new building) and activity 2. (Renovation of existing building) are covered by the description in the Disclosures Delegated Act towards the environmental objective of climate change mitigation.\n\n\n\nTable 3. Example of turnover table\n\n\n\n\n\nTable 4. Total turnover, capital expenditures, operating expenses\n\n\n\n\n\nTable 5. Additional information on taxonomy-alignment of activities\n\n\n\\[\n\\begin{aligned}\n\\text{Taxonomy-Eligible Quota} &= \\frac{\\text{Taxonomy-Eligible total}}{\\text{Total for segments (Revenue/CAPEX/OPEX) }}\\\\\n\\text{Taxonomy-Aligned Quota} &= \\frac{\\text{Taxonomy-Aligned total}}{\\text{Total for segments (Revenue/CAPEX/OPEX) }}\n\\end{aligned}\n\\]\n\n[i]. Let’s calculate the taxonomy quotas for taxonomy-eligible activities (eligible turnover, CapEx, OpEx) per activity using the information in Table 4. \n[ii]. Let’s calculate the taxonomy quotas for taxonomy-eligible activities (eligible turnover, CapEx, OpEx) for the whole firm using the information in Table 4.\n\n\\[\n\\begin{aligned}\nSUM_{activities} &= 0.2818 + 0.2364 +.. \\text{for Revenue} \\\\\nSUM_{activities} &= 0.155 + 0.35 +.. \\text{for CAPEX} \\\\\nSUM_{activities} &= 0.1316 + 0.3184 +.. \\text{for OPEX}\n\\end{aligned}\n\\]\n\n[iii]. Let’s calculate the taxonomy quotas for taxonomy-aligned per activity using the information in Table 4 and 5, assuming that the CEO wishes to report quotas that are as high as possible. \n[iv]. Let’s calculate the taxonomy quotas for taxonomy-aligned activities for the whole firm using the information in Table 4 and 5.\n\n\\[\n\\begin{aligned}\nSUM_{activities} &= 0.2682 + 0.1568 +.. \\text{for Revenue} \\\\\nSUM_{activities} &= 0.1175 + 0.338 +.. \\text{for CAPEX} \\\\\nSUM_{activities} &= 0.1 + 0.24 +.. \\text{for OPEX}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "whiteboard03/POST/project_4/index.html#d.-potential-for-double-counting",
    "href": "whiteboard03/POST/project_4/index.html#d.-potential-for-double-counting",
    "title": "EU-Taxonomy Analysis",
    "section": "[D]. Potential for Double-Counting",
    "text": "[D]. Potential for Double-Counting\nIn Annex 1, par 1.2. Disclosure Delegated Acts, firms have to explain how they avoid double-counting in the allocation of Revenue, CAPEX, OPEX across activities.\nThe subsidiary of Gensler AG, Sasaki, will be consolidated in the nonfinancial report of Gensler AG. Sasaki is engaged in similar economic activities as its parent company. Since Gensler AG only has contracts with customers where both new buildings are constructed and existing buildings are renovated simultaneously, the taxonomy-aligned turnover, CapEx, and OpEx are allocated towards both economic activities by the financial reporting department. Due to differences in the DNSH check, the taxonomy-aligned values partly vary between the two economic activities even though they stem from the same underlying contracts. Table 6 reports the values for both economic activities separately as well as for the total firm.\n\n\n\nTable 6. Taxonomy-aligned turnover, CapEx, OpEx of Sasaki\n\n\nLet’s say the CEO of parent company Gensler AG wishes to report taxonomy quotas that are as high as possible. Now let’s calculate the taxonomy-aligned turnover, CapEx, and OpEx for the subsidiary Sasaki according to the EU taxonomy regulation and considering the CEO’s objective.\n\n\n\nTable 7. Additional information on Sasaki\n\n\n\n\n\n\n\n\nNote\n\n\n\nissue: double-counting possible (not allowed)\n\nSolution I. \nSolution II. \nAllocation of activities only to objective construction of new buildings yields the highest taxonomy quotas as requested by the CEO. The reported values are subject to the individual allocation decision made by firm management and not objectively justified by the EU taxonomy regulation."
  },
  {
    "objectID": "whiteboard03/POST/project_4/index.html#bibliography",
    "href": "whiteboard03/POST/project_4/index.html#bibliography",
    "title": "EU-Taxonomy Analysis",
    "section": "Bibliography",
    "text": "Bibliography\n[1] Climate Delegated Act: Commission Delegated Regulation (EU) 2021/2139. Available at https://eurlex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:02021R2139-20250108 (consolidated version, January 8, 2025).\n[2] Environmental Delegated Act: Commission Delegated Regulation (EU) 2023/2486. Available at https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L_202302486 (November 11, 2023).\n[3] Disclosures Delegated Act: Commission Delegated Regulation (EU) 2021/2178. Available at https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:02021R2178-20240101 (consolidated version, January 1, 2024)."
  }
]